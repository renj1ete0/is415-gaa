---
title: "Take-Home Exercise 03: Predicting HDB Public Housing Resale Prices Using Geographically Weighted Methods"
description: "Predicting HDB Public Housing Resale Prices Using Geographically Weighted Methods trained using Jan 2021 to Dec 2022 and tested with Jan to Feb 2023"
author: "Teo Ren Jie"
date: "3/12/2023"
date-modified: "4/13/2023"
number-sections: true
categories: ["Take-Home Exercise"]
title-block-banner: true
image: Take-Home_Ex03/preview.png
execute:
  message: false
  warning: false
---

# Overview

## Setting the Scene

Singapore's Housing Development Board (HDB) flats are quite ubiquitous, being the most common housing type Singaporeans live in.

In recent months, questions about [HDB affordability](https://www.channelnewsasia.com/singapore/bto-flats-hdb-affordable-prices-development-costs-3127281) has been raised, which led to PM Lee's assurance on affordability housing for all in [Budget 2023](https://www.straitstimes.com/singapore/housing/singaporeans-will-not-have-to-worry-about-having-an-affordable-home-to-call-their-own-pm-lee). Yet, in the past year, [more million dollar HDBs](https://www.reuters.com/markets/asia/singapore-sees-rise-million-dollar-public-housing-2022-08-31/)were resold.

So how could we understand and predict HDB resale prices? Are they due to the location and proximity to points of interest? Or are they due to the type and size of HDB? Could there also be unknown increased demand in certain geographical araes in Singapore that leads to many of these million dollar HDBs?

## Tasks

We will work on constructing and comparing an OLS Multiple Linear Regression Model and Geographic Weighted Random Forest Model to predict 5-room HDB prices for the month of January and February 2023 in Singpaore.

The training data will consist of 5-room HDB resale data between January 2021 to December 2022.

# Getting Started

## Installing and Loading Packages

Next, pacman assists us by helping us load R packages that we require, `sf`, `tidyverse` and funModeling.

```{r}
pacman::p_load(readxl, sf, tidyverse, tmap, sfdep, rvest, httr, jsonlite, onemapsgapi, ggpubr, olsrr, GWmodel, SpatialML, Metrics, ggplot2, plotly, spdep)
```

The following packages assists us to accomplish the following:

-   *readxl* assists us in importing `.xlsx` aspatial data without having to convert to `.csv`

-   *sf* helps to import, manage and process vector-based geospatial data in R

-   *tidyverse* which includes *readr* to import delimited text file, *tidyr* for tidying data and *dplyr* for wrangling data

-   *tmap* provides functions to allow us to plot high quality static or interactive maps using leaflet API

-   *rvest, httr*, *jsonlite*, *onemapsgapi* helps us to handle requests, convert json data and load datasets directly from onemap

-   *ggpubr*, *ggplot2, plotly* helps us with plotting the interactive and beautiful graphs

-   *olsrr* helps with Ordinary Least Squares Linear Regression

-   *Gwmodel* and *SpatialML* helps with the creation and prediction of Geographically Weighted Random Forest Machine Learning model

-   *Metrics* helps to calculate the RMSE (Root Means Squared Error) metrics for the prediction

-   *spdep* helps with calculation with nearest neighbours to provide data to calculate the Moran I's statistics to determine clustering effects

## Data Acquisition

The following datasets would be used to create the predictive models using conventional OLS and GWR methods for HDB Resale Prices.

+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Dataset Type | Dataset Name                            | Remarks                                                                                                                                                              | Source                                                                                                                   |
+==============+=========================================+======================================================================================================================================================================+==========================================================================================================================+
| Geospatial   | URA Master Plan 2019 Subzone Boundary   | For visualisation purposes and extract Central Area                                                                                                                  | Prof Kam                                                                                                                 |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Aspatial     | HDB Resale Flat Prices                  |                                                                                                                                                                      | [data.gov.sg](https://data.gov.sg/dataset/resale-flat-prices)                                                            |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Aspatial     | HDB MUP/HIP Status                      | Manual Web Scraping                                                                                                                                                  | [hdb.gov.sg](https://services2.hdb.gov.sg/webapp/BB33RESLSTATUS/BB33PReslStatusEnq.jsp)                                  |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Aspatial     | Shopping Malls                          | Manual web scraping                                                                                                                                                  | [wikipedia.org : List of Shopping Malls in Singapore](https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore) |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Childcare                               |                                                                                                                                                                      | [onemap.sg Themes](https://www.onemap.gov.sg/docs/#themes)                                                               |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Kindergartens                           |                                                                                                                                                                      | [onemap.sg Themes](https://www.onemap.gov.sg/docs/#themes)                                                               |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Eldercare                               |                                                                                                                                                                      | [onemap.sg Themes](https://www.onemap.gov.sg/docs/#themes)                                                               |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Foodcourt/Hawker                        |                                                                                                                                                                      | [onemap.sg Themes](https://www.onemap.gov.sg/docs/#themes)                                                               |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Supermarket                             |                                                                                                                                                                      | [onemap.sg](onemap.sg)                                                                                                   |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Current and Future MRT/LRT Stations     | Excludes Cross Region Line Punggol Branch                                                                                                                            | [data.gov.sg](https://data.gov.sg/dataset/master-plan-2019-rail-line-layer)                                              |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| \-           | Future MRT Station (CRL Punggol Branch) | Manually merge into MRT/LRT Station Dataset                                                                                                                          | [wikipedia.org : Elias MRT Stn](https://en.wikipedia.org/wiki/Elias_MRT_station)\                                        |
|              |                                         |                                                                                                                                                                      | [wikipedia.org : Riveria MRT Stn](https://en.wikipedia.org/wiki/Riviera_MRT/LRT_station)                                 |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | MRT/LRT Railway Line                    | Filter elevated sections of MRT line                                                                                                                                 | [data.gov.sg](https://data.gov.sg/dataset/master-plan-2019-rail-line-layer)                                              |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Bus Stops                               |                                                                                                                                                                      | [datamall.lta.gov.sg](https://datamall.lta.gov.sg/content/dam/datamall/datasets/Geospatial/BusStopLocation.zip)          |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Parks                                   | We consider the rail corridor, nature reserves and parks as parks as they are for leisure purposes.                                                                  | [data.gov.sg](https://data.gov.sg/dataset/nparks-parks-and-nature-reserves)                                              |
|              |                                         |                                                                                                                                                                      |                                                                                                                          |
|              |                                         | Also, we will prefer polygons of parks as we can calculate the actual proximity to the edges of the parks instead of to an arbitary point in the centre of the park. |                                                                                                                          |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Primary Schools                         | Requires special handling                                                                                                                                            | onemap.sg json                                                                                                           |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Aspatial     | CHAS Clinics                            | Extracted using Excel from PDF                                                                                                                                       | [chas.sg](https://www.chas.sg/clinic-locator)                                                                            |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+

## Data Fields

The data fields we are looking to incorporate and work with in our predictive models includes:

::: panel-tabset
## Structural Factors

-   Area of the unit

-   Floor level

-   Remaining lease

-   Age of the unit

-   Main Upgrading Program (MUP) completed

    -   Extracted MUP and Home Improvment Programme (HIP) data from HDB website

    -   For HDB units that has received HIP, their home value may be affected positively than a similar aged flat that has not received it

-   Flat Model (eg. DBSS/Standard/Premium)

    -   Design Build Sell Scheme (DBSS) flats may call for a higher value than regular HDB flats as they are designed, build and sold by 3rd party developers although they are still HDB Flats. They are supposed to be better than premium flats

    -   Premium flats which come with pre-installed fittings and furnishings over standard apartments which comes with none

    -   Reference: https://www.teoalida.com/singapore/hdbflattypes/

-   Flat Multi-storey (Maisonette or Loft)

    -   Some homeowners may prefer multi-story HDBs over single-story ones

## Locational Factors

-   Proximity to CBD

-   Proximity to eldercare

-   Proximity to foodcourt/hawker centres

-   Proximity to MRT

-   Proximity to park

-   Proximity to good primary school

-   Proximity to shopping mall

-   Proximity to supermarket

-   Numbers of kindergartens within 350m

-   Numbers of childcare centres within 350m

-   Numbers of bus stop within 350m

-   Numbers of primary school within 1km

-   Proximity to Overhead MRT Line \[noise concern\]

    -   The closer a HDB unit is to the MRT track, the home value might be affected due to noise concerns. We measure the proximity of HDB units using its euclidean distance to the closest part of the MRT track if it is less than 300metres away.

-   Proximity to Overhead LRT Line (similar to MRT line)

-   Number of Future MRT stops within 800m (10min walk)

    -   Here, I want to explore how the resale values of HDBs could be affected by future MRT stations that are announced but not yet built. Home owners may be enticed to buy houses near future MRT lines in hopes that the house values will increase and also due to increased connectivity

-   Number of LRT Stops within 350m

    -   The metric is necessary as LRT serves as a feeder within the town and is typically used short-haul vs MRT which is between various towns. The 350m metric is derived from Bus Stops differentiates the weight between a LRT stop and MRT stop especially if the LRT stop is far away from the MRT stop in towns such as Sengkang, Punggol and Pasir Ris
:::

# Data Wrangling: Geospatial Data

There are two categories of datasets we will need for our analysis, these includes:

-   Datasets that has been downloaded - These files are already downloaded into a local location

-   Datasets that are retrieved over API - We need to obtain the datasets using API Calls

## Importing / Retrieving / Obtaining Data

### Retrieving Data from API Calls

There are some data that we need to retreive using API calls from [onemap.sg](onemap.sg). OneMap offers additional data from different government agencies through [Themes](https://www.onemap.gov.sg/docs/#themes). For R, the [onemapsgapi](https://cran.r-project.org/web/packages/onemapsgapi/onemapsgapi.pdf) package helps us with the API calls with onemap.sg servers to obtain the data we require.

Using onemapsgapi is pretty simple as shown below:

```{r eval=FALSE}
token <- "" # enter authentication token obtained from onemap
search_themes(token, "<searchterm>") %>% print(n=Inf)
tibble <- get_theme(token, "<queryname>")
sf <- st_as_sf(tibble, coords=c("Lng", "Lat"), crs=4326)
```

-   *search_themes()* - Search for various thematic layers provided by onemap (eg. Parks). A tibble dataframe will be provided with more details of the layer, such as the `THEMENAME`, `QUERYNAME`, `ICON`, `CATEGORY` and `THEME_OWNER`

-   *get_theme()* - Using the desired theme's `QUERYNAME` obtained from *search_themes()*, we can obtain the thematic data in a tibble dataframe. We will need to use st_as_sf to specify the `Lat`, `Lng` and crs to obtain it as a sf dataframe.

Listed below are a list of layers we need to obtain:

-   Childcare

-   Kindergartens

-   Eldercare

-   Foodcourt/Hawker Centres

In the code block below, we will assume to have used *search_themes()* to pick the specific themes we want, to load them. The justification will be listed below.

::: panel-tabset
## Obtain Data

Childcare

```{r eval=FALSE}
childcare_tibble <- get_theme(token, "childcare")
childcare_sf <- st_as_sf(childcare_tibble, coords=c("Lng", "Lat"), crs=4326)
```

Kindergartens

```{r eval=FALSE}
kindergartens_tibble <- get_theme(token, "kindergartens")
kindergartens_sf <- st_as_sf(kindergartens_tibble, coords=c("Lng", "Lat"), crs=4326)
```

Eldercare

```{r eval=FALSE}
eldercare_tibble <- get_theme(token, "eldercare")
eldercare_sf <- st_as_sf(eldercare_tibble, coords=c("Lng", "Lat"), crs=4326)
```

Foodcourt/Hawker Centre

```{r eval=FALSE}
hawker_tibble <- get_theme(token, "hawkercentre_new")
hawker_sf <- st_as_sf(hawker_tibble, coords=c("Lng", "Lat"), crs=4326)
```

## RDS Scripts (Save)

```{r eval=FALSE}
write_rds(childcare_sf, "Take-Home_Ex03/rds/childcare_sf.rds")
write_rds(kindergartens_sf, "Take-Home_Ex03/rds/kindergartens_sf.rds")
write_rds(eldercare_sf, "Take-Home_Ex03/rds/eldercare_sf.rds")
write_rds(hawker_sf, "Take-Home_Ex03/rds/hawker_sf.rds")
```
:::

### Obtaining Schools Data

Obtaining school data from OneMap is a bit tricky, it was not available through OneMap themes or a download link through the OneMap website. However, through clicking through the Query Schools function on the map using using 'Inspect Element', we could see that a GET request is called to obtain the map data as json (as seen in the screenshot below):

![](Take-Home_Ex03/geospatial/onemap-schools-1.png)

By opening the link, we could see that it is an undocumented public API that OneMap uses to retrieve map data regarding Primary Schools. The results are in json as shown below:

![](Take-Home_Ex03/geospatial/onemap-schools-2.png)

The data has been downloaded and will be processed into tibble format using *json_lite* *fromJSON()* which will import the JSON file and convert it into tibble dataframe.

```{r}
schools_tibble <- fromJSON("Take-Home_Ex03/geospatial/retrieveAllSchools.json")[["SearchResults"]]
glimpse(schools_tibble)
```

As we can see the want to exclude the column `PageCount` and the first row as it is not relavant to our dataset. The code chunk below will perform the above for us:

```{r}
schools_tibble <- select(schools_tibble,-"PageCount")
schools_tibble <- schools_tibble[-1,]
```

Next, we will convert the tibble dataframe to sf dataframe. Since X and Y coordinates are provided for us (SVY21) in the columns `SCH_Y_ADDR` and `SCH_X_ADDR`, we will use them instead of the `Lng` and `Lat` as SVY21 (Projected Coordinate System) will allow us to perform our analysis directly.

```{r}
schools_sf_3414 <- st_as_sf(schools_tibble, coords=c("SCH_X_ADDR", "SCH_Y_ADDR"), crs=3414)
```

Now, we will save the data imported as RDS file format (R Data Serialisation).

```{r eval=FALSE}
write_rds(schools_sf_3414, "Take-Home_Ex03/rds/schools_sf_3414.rds")
```

### Importing Geospatial Data

::: panel-tabset
## Downloaded

Master Plan Subzone 2019

```{r}
mpsz = st_read(dsn = "Take-Home_Ex03/geospatial", layer="MPSZ-2019")
```

Current and Future MRT/LRT Stations

```{r}
geo_mrt_lrt_stn = st_read(dsn = "Take-Home_Ex03/geospatial/master-plan-2019-rail-station-layer-kml.kml")
```

MRT/LRT Railway Line

```{r}
geo_railway_line = st_read(dsn = "Take-Home_Ex03/geospatial/rail-line.kml")
```

Bus Stops

```{r}
geo_bus_stop = st_read(dsn = "Take-Home_Ex03/geospatial", layer="BusStop")
```

Parks

```{r}
geo_parks = st_read(dsn = "Take-Home_Ex03/geospatial/nparks-parks-and-nature-reserves-kml.kml")
```

Supermarket

```{r}
geo_supermarkets = st_read(dsn = "Take-Home_Ex03/geospatial", layer="Supermarkets")
```

## Other Data (RDS)

```{r}
geo_schools <- read_rds("Take-Home_Ex03/rds/schools_sf_3414.rds")
geo_childcare <- read_rds("Take-Home_Ex03/rds/childcare_sf.rds")
geo_eldercare <- read_rds("Take-Home_Ex03/rds/eldercare_sf.rds")
geo_hawker <- read_rds("Take-Home_Ex03/rds/hawker_sf.rds")
geo_kindergartens <- read_rds("Take-Home_Ex03/rds/kindergartens_sf.rds")
```
:::

## Transforming Coordinate Systems

For datasets in `WGS84` Geodetic Coordinate System, we need to convert them to `SVY21` Projected Coordinate System to perform our analysis. Inferring form the information above, we will use the code chunk below to confirm all of them.

::: panel-tabset
## Transform

We use *st_zm()* on the kml datasets to remove the Z dimensions which will cause issues with analysis later as XY and XYZ data do not work well with one another.

```{r}
mpsz <- st_transform(mpsz,3414)
geo_mrt_lrt_stn <- st_transform(st_zm(geo_mrt_lrt_stn),3414)
geo_railway_line <- st_transform(st_zm(geo_railway_line),3414)
geo_parks <- st_transform(st_zm(geo_parks),3414)
geo_supermarkets <- st_transform(geo_supermarkets,3414)
geo_childcare <- st_transform(geo_childcare,3414)
geo_eldercare <- st_transform(geo_eldercare,3414)
geo_hawker <- st_transform(geo_hawker,3414)
geo_kindergartens <- st_transform(geo_kindergartens,3414)
```

## Check

Bus Stop

```{r}
st_crs(geo_bus_stop)
```

Oh, the CRS was not set properly and reflected as EPSG:9001

```{r}
geo_bus_stop <- st_set_crs(geo_bus_stop, 3414)
st_crs(geo_bus_stop)
```

Done!

Master Plan Subzone 2019

```{r}
mpsz
```

Current and Future MRT/LRT Stations

```{r}
geo_mrt_lrt_stn
```

MRT/LRT Railway Line

```{r}
geo_railway_line
```

Parks

```{r}
geo_parks
```

Supermarkets

```{r}
geo_supermarkets
```

Childcare

```{r}
geo_childcare
```

Eldercare

```{r}
geo_eldercare
```

Hawker

```{r}
geo_hawker
```

Kindergartens

```{r}
geo_kindergartens
```
:::

Great! Now everything is in `SVY21` Projected Coordinate System.

## Transform Datasets

### Fixing Master Plan Subzone Boundary Geometries

As we recall for exercises in class, there are issues with invalid geometries in the dataset.

```{r}
length(which(st_is_valid(mpsz) == FALSE))
```

Here, we will fix it by using *st_make_valid()*

```{r}
mpsz <- st_make_valid(mpsz)
length(which(st_is_valid(mpsz) == FALSE))
```

Great, its fixed!

### Fixing KML Data

When we look at the MRT/LRT Station and Railway Line stations, we find that the labels are `KML_1`, `KML_2`, etc which are not useful for our analysis.

```{r}
glimpse(geo_mrt_lrt_stn)
```

```{r}
glimpse(geo_railway_line)
```

Here, we can see that many of the attributes are nested in a HTML format under the `description` column, We will now fix the `KML` imported data for MRT/LRT Station and Railway Line datasets so we can access the attributes to filter it effectively for our further analysis. The code referenced is from [StackOverflow](https://stackoverflow.com/questions/50775357/how-to-read-in-kml-file-properly-in-r-or-separate-out-lumped-variables-into-col):

```{r}
attributes <- lapply(X = 1:nrow(geo_mrt_lrt_stn), 
                     FUN = function(x) {

                       geo_mrt_lrt_stn %>% 
                         slice(x) %>%
                         pull(Description) %>%
                         read_html() %>%
                         html_node("table") %>%
                         html_table(header = TRUE, trim = TRUE, dec = ".", fill = TRUE) %>%
                         as_tibble(.name_repair = ~ make.names(c("Attribute", "Value"))) %>% 
                         pivot_wider(names_from = Attribute, values_from = Value)

                     })
geo_mrt_lrt_stn <- 
  geo_mrt_lrt_stn %>%
  bind_cols(bind_rows(attributes)) %>%
  select(-Description)

glimpse(geo_mrt_lrt_stn)
```

```{r}
attributes <- lapply(X = 1:nrow(geo_railway_line), 
                     FUN = function(x) {

                       geo_railway_line %>% 
                         slice(x) %>%
                         pull(Description) %>%
                         read_html() %>%
                         html_node("table") %>%
                         html_table(header = TRUE, trim = TRUE, dec = ".", fill = TRUE) %>%
                         as_tibble(.name_repair = ~ make.names(c("Attribute", "Value"))) %>% 
                         pivot_wider(names_from = Attribute, values_from = Value)

                     })
geo_railway_line <- 
  geo_railway_line %>%
  bind_cols(bind_rows(attributes)) %>%
  select(-Description)

glimpse(geo_railway_line)
```

Great now we have extracted the attributes into its own columns where we can use it for further analysis.

### Transforming and Modifying MRT/LRT Station Data

Let us view `geo_mrt_lrt_stn` data on a map and the table and fix any NA values we might find:

::: panel-tabset
## Map

```{r}
tmap_mode("plot") +
  tm_shape(mpsz) +
  tm_polygons("REGION_N", alpha = 0.1, border.alpha = 0.1) +
  tm_shape(geo_mrt_lrt_stn) +
  tm_fill("RAIL_TYPE", palette =c("red", "blue")) +
  tm_layout(legend.position = c("right", "bottom"), 
          title= 'MRT/LRT Stations in Singapore', 
          title.position = c('right', 'top'))
```

## Glimpse

```{r}
glimpse(geo_mrt_lrt_stn)
```

## Checking and fixing NA Values

Filter and view data

```{r}
geo_stn_na <- filter(geo_mrt_lrt_stn,NAME == "")
geo_stn_na
```

View on a map

```{r}
tmap_mode("view") +
  tm_shape(mpsz) +
  tm_polygons("REGION_N", alpha = 0.1, border.alpha = 0.1) +
  tm_shape(geo_stn_na) +
  tm_fill("Name", palette =c("red", "blue"), popup.vars=c("NAME" = "NAME"))
```

From the map and data above, we can see 9 stations has its names missing as shown below:

| Name (KML Name) | NAME (Station Name)          |
|-----------------|------------------------------|
| kml_74          | Imbiah (Sentosa Express)     |
| kml_75          | Beach (Sentosa Express)      |
| kml_77          | Downtown (DTL)               |
| kml_80          | Chinatown (DTL)              |
| kml_92          | Newton (DTL)                 |
| kml_97          | Maxwell (TEL)                |
| kml_107         | Waterfront (Sentosa Express) |
| kml_150         | Marina East (TEL)            |
| kml_203         | Orchard Boulevard (TEL)      |

We don't want the Sentosa Express data as it serves more for leisure purpose. We will drop it from the dataframe later.

Fixing Data

```{r}
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_74", "NAME"] <- "IMBIAH"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_75", "NAME"] <- "BEACH"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_77", "NAME"] <- "DOWNTOWN"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_80", "NAME"] <- "CHINATOWN"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_92", "NAME"] <- "NEWTON"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_97", "NAME"] <- "MAXWELL"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_107", "NAME"] <- "WATERFRONT"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_150", "NAME"] <- "MARINA SOUTH"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_203", "NAME"] <- "ORCHARD BOULEVARD"
```
:::

There are a few steps to obtaining the data in the format we want.

We want the data in three dataframes:

1.  Existing MRT stations - North South Line, East West Line, Changi Airport Line, North East Line, Circle Line, Downtown Line, Thomson East Coast Line 1, 2 and 3
2.  Existing LRT stations - Bukit Panjang LRT, Sengkang LRT, Punggol LRT
3.  Future MRT stations - Thomson East Coast Line 4, 5, Jurong Region Line, Cross Island Line 1, Punggol Extension (we need to manually insert the stations)

The reason why Cross Island Line 2 was not included is that it is only [announced on 20 Sep 2022](https://www.lta.gov.sg/content/ltagov/en/newsroom/2022/9/news-releases/cross-island-line-phase-2.html) which is outside of our model data range. Hence, those stations would not have affected the housing prices in any way. We also want to exclude stations that do not have a definite opening date (Bukit Brown, Marina South and Mount Pleasant).

There are also a few other hurdles we need to go through:

1.  Interchange MRT stations have multiple polygons and records, we need to merge them
2.  For our analysis, we want to convert the polygons to points to be able to perform our analysis.

### Extraction of Data into Different DataFrames

::: panel-tabset
## Future MRT

```{r}
FUTURE_MRT = c("CHOA CHU KANG WEST", "TENGAH", "TENGAH PLANTATION", "TENGAH PARK", "BUKIT BATOK WEST", "TOH GUAN", "JURONG TOWN HALL", "PANDAN RESERVOIR", "HONG KAH", "CORPORATION", "JURONG WEST", "BAHAR JUNCTION", "GEK POH", "TAWAS", "NANYANG GATEWAY", "NANYANG CRESCENT", "PENG KANG HILL", "ENTERPRISE", "TUKANG", "JURONG HILL", "JURONG PIER", "FOUNDERS' MEMORIAL", "TANJONG RHU", "KATONG PARK", "TANJONG KATONG", "MARINE PARADE", "MARINE TERRACE", "SIGLAP", "BAYSHORE", "BEDOK SOUTH", "SUNGEI BEDOK", "XILIN", "AVIATION PARK", "LOYANG", "PASIR RIS EAST", "TAMPINES NORTH", "DEFU", "SERANGOON NORTH", "TAVISTOCK", "TECK GHEE", "HUME", "KEPPEL", "CANTONMENT", "PRINCE EDWARD ROAD", "PUNGGOL COAST")

EXCLUDE = c("MARINA SOUTH", "BUKIT BROWN", "MOUNT PLEASANT", "WATERFRONT", "BEACH", "IMBIAH")

geo_mrt_future <- geo_mrt_lrt_stn %>%
  filter(NAME %in%  FUTURE_MRT)

glimpse(geo_mrt_future)
```

Looks correct! We have 44 unique future MRT stations that are new (excludes new interchanges with existing lines), 1 unique station is Sungei Bedok which is an interchange on TEL and DTL, hence, 44 records.

## Existing LRT

```{r}
geo_lrt <- geo_mrt_lrt_stn %>%
  filter(RAIL_TYPE == "LRT") %>% filter(!NAME %in% EXCLUDE)

glimpse(geo_lrt)
```

Looks correct! We have 42 LRT stations in Singapore.

## Existing MRT

```{r}
geo_mrt_existing <- geo_mrt_lrt_stn %>%
  filter(RAIL_TYPE == "MRT") %>% filter(!NAME %in% EXCLUDE) %>% filter(!NAME %in% FUTURE_MRT)
geo_mrt_existing
```

By looking through the dataframe, the data looks correct!
:::

#### Merging Polygons for Data Frame

For `geo_mrt` which contains data of existing MRT stations, there are interchange stations which has seperate polygons. For example, Dhoby Ghaut MRT station is an interchange between 3 lines and hence has 3 polygons and records as seen below:

```{r}
filter(geo_mrt_existing, NAME == "DHOBY GHAUT INTERCHANGE")
```

We want to merge the records to obtain a single spatial point for each MRT station. Below, we will identify the interchange stations and merge their records and polgons manually.

::: panel-tabset
## Merge Functions

Function to merge 2 and 3 rows respectively

```{r}
merge_2 <- function(df, kml_1, kml_2){
  operation <- st_union(filter(df,  Name == kml_1), filter(df,  Name == kml_2))
  operation <- select(operation, "geometry")
  df[df$Name == kml_1, "geometry"] <- operation
  df <- subset(df, Name != kml_2)
  
  return(df)
}

merge_3 <- function(df, kml_1, kml_2, kml_3){
  operation <- st_union(filter(df,  Name == kml_1), filter(df,  Name == kml_2))
  operation <- select(operation, c(0:6, "geometry"))
  operation <- st_union(operation, filter(df,  Name == kml_3))
  operation <- select(operation, "geometry")
  df[df$Name == kml_1, "geometry"] <- operation
  df <- subset(df, Name != kml_2)
  df <- subset(df, Name != kml_3)
  
  return(df)
}
```

## Merging Polygons

The polygons are merged for the stations as indicated in the code block

```{r}
# ANG MO KIO
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_20", "kml_236")
# BISHAN
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_43", "kml_247")
# BOON LAY
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_180", "kml_205")
# BOTANIC GARDENS
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_210", "kml_211")
# BONUA VISTA
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_227", "kml_228")
# CALDECOTT
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_231", "kml_232")
# CHINATOWN
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_80", "kml_165")
# CHOA CHU KANG
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_118", "kml_187")
# DHOBY GHAUT
geo_mrt_existing <- merge_3(geo_mrt_existing, "kml_156", "kml_157", "kml_158")
# EXPO
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_108", "kml_174")
# HARBOURFRONT
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_58", "kml_59")
# HOUGANG
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_11", "kml_245")
# JURONG EAST
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_135", "kml_136")
# LITTLE INDIA
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_160", "kml_161")
# MACPHERSON
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_222", "kml_223")
# MARINA BAY
geo_mrt_existing <- merge_3(geo_mrt_existing, "kml_68", "kml_78", "kml_147")
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_68", "kml_148")
# NEWTON
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_72", "kml_92")
# ORCHARD
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_98", "kml_154")
# OUTRAM PARK
geo_mrt_existing <- merge_3(geo_mrt_existing, "kml_100", "kml_151", "kml_251")
# PASIR RIS
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_239", "kml_243")
# PAYA LEBAR
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_153", "kml_212")
# SERANGOON
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_8", "kml_10")
# STEVENS
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_105", "kml_209")
# TAMPINES
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_35", "kml_166")
# WOODLANDS
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_64", "kml_169")
# BUGIS
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_81", "kml_70")
```

That's right, we have 134 unique existing MRT stations
:::

#### Converting Spatial Polygons to Spatial Points

```{r}
geo_mrt_existing <- st_centroid(geo_mrt_existing)
geo_lrt <- st_centroid(geo_lrt)
geo_mrt_future <- st_centroid(geo_mrt_future)
```

#### Insert Cross Island Line Punggol Future Stations

The Master Plan 2019 MRT/LRT Station data excludes the Cross Island Line Punggol Stations, so we have to add them. 2 New MRT stations (that are not an existing interchange station with existing lines needs to be added). These are: [Riveria](https://en.wikipedia.org/wiki/Riviera_MRT/LRT_station)and [Elias](https://en.wikipedia.org/wiki/Elias_MRT_station)

```{r}
new_df <- data.frame(Name = "kml_998", GRND_LEVEL = "UNDERGROUND", RAIL_TYPE = "MRT", NAME = "ELIAS", INC_CRC = "", FMEL_UPD_D = "", lng = "103.984", lat = "1.384")
new_df_coords <- st_as_sf(new_df, coords = c("lng", "lat"), crs=4326) 
new_df_coords <- new_df_coords %>% st_transform(3414)
geo_mrt_future <- rbind(new_df_coords, geo_mrt_future)

new_df <- data.frame(Name = "kml_999", GRND_LEVEL = "UNDERGROUND", RAIL_TYPE = "MRT", NAME = "RIVERIA", INC_CRC = "", FMEL_UPD_D = "", lng = "103.916772", lat = "1.394439")
new_df_coords <- st_as_sf(new_df, coords = c("lng", "lat"), crs=4326) 
new_df_coords <- new_df_coords %>% st_transform(3414)
geo_mrt_future <- rbind(new_df_coords, geo_mrt_future)
```

### Verifying MRT/LRT Data

```{r}
tmap_mode("plot") +
  tm_shape(mpsz) +
  tm_polygons("REGION_N", alpha = 0.05, border.alpha = 0.05) +
  tm_shape(geo_mrt_existing) +
  tm_dots("RAIL_TYPE", palette = "darkgreen", title = "Existing MRT", size = 0.02) +
  tm_shape(geo_lrt) +
  tm_dots("RAIL_TYPE", palette = "blue", title = "Existing LRT", size = 0.02) +
  tm_shape(geo_mrt_future) +
  tm_dots("RAIL_TYPE", palette = "red", title = "Future MRT", size = 0.02) +
  tm_layout(legend.position = c("right", "bottom"), 
            title= 'MRT/LRT Stations in Singapore', 
            title.position = c('right', 'top'))
```

Everything looks to be plotted correctly.

### Transforming Railway Line

```{r}
tmap_mode("plot") +
  tm_shape(mpsz) +
  tm_polygons("REGION_N", alpha = 0.05, border.alpha = 0.05) +
  tm_shape(geo_railway_line) +
  tm_lines(c("GRND_LEVEL", "RAIL_TYPE"), palette = c("red", "blue", "darkgreen")) +
  tm_layout(legend.position = c("right", "bottom"), 
            title= 'Railway Line in Singapore', 
            title.position = c('right', 'top'))
```

As we can see from our tmap plot above, the dataset contains:

1.  `GRND_LEVEL` - Whether the track segment is above or underground
2.  `RAIL_TYPE` - Whether the track belongs to `LRT`, `MRT` or `RAILWAY` (KTM train)

Do note that since the data is extracted from URA Master Plan 2019 Rail Line, we will be able to see all current and future rail lines (Thomson East Coast Lines Stages 4, 5, Cross Island Line 1, Jurong Region Line).

For our analysis, we only want the above ground segments, seperated by `RAIL_TYPE` but excluding KTM data, as generally above ground segments affects residents the most. The reason why we seperate it by `RAIL_TYPE` is that LRT makes lesser noise than MRT and may not adversely impact housing prices as much as MRT. For MRTs, [NUS researchers](https://ireus.nus.edu.sg/mrt-and-property-value/) found that housing values were impacted by noise.

The rationale of including future aboveground lines like the Jurong Region Line in our analysis is that housing prices could be affected by the construction or announcement of future MRT lines which may cause housing prices to fall.

#### Splitting MRT/LRT Datasets

::: panel-tabset
## Filter

MRT

```{r}
geo_rail_mrt_above <- geo_railway_line %>% filter(GRND_LEVEL == "ABOVEGROUND") %>% filter(RAIL_TYPE == "MRT")
```

LRT

```{r}
geo_rail_lrt_above <- geo_railway_line %>% filter(GRND_LEVEL == "ABOVEGROUND") %>% filter(RAIL_TYPE == "LRT")
```

## Check

MRT

```{r}
glimpse(geo_rail_mrt_above)
```

LRT

```{r}
glimpse(geo_rail_lrt_above)
```
:::

#### Verifying MRT/LRT Aboveground Railway Line

```{r}
tmap_mode("plot") +
  tm_shape(mpsz) +
  tm_polygons("REGION_N", alpha = 0.05, border.alpha = 0.05) +
  tm_shape(geo_rail_mrt_above) +
  tm_lines("RAIL_TYPE", palette = "red") +
  tm_shape(geo_rail_lrt_above) +
  tm_lines("RAIL_TYPE", palette = "blue") +
  tm_layout(legend.position = c("right", "bottom"), 
            title= 'MRT/LRT Track Line in Singapore', 
            title.position = c('right', 'top'))
```

### Transform Parks Dataset

Let us view our parks dataset

```{r}
glimpse(geo_parks)
```

```{r}
tmap_mode("plot") +
  tm_shape(mpsz) +
  tm_polygons("REGION_N", alpha = 0.05, border.alpha = 0.05) +
  tm_shape(geo_parks) +
  tm_fill("darkgreen") +
  tm_layout(legend.position = c("right", "bottom"), 
            title= 'Parks in Singapore', 
            title.position = c('right', 'top'))
```

Firstly, as we recognise that parks comes in different shapes and sizes. Parks like Punggol Waterway Park are long by nature and spans the entire width of Punggol. Hence, using a Spatial Points by obtaining its centroid is not the most accurate as the entire length is a park. Hence, we opt to use the park polygon instead.

Our data is in the `MULTIPOLYGON` format. As we want to calculate the proximity from homes to the edges of parks, we need to convert it to `LINESTRING`. The code block uses *st_cast()* to help us cast the format from `MULTIPOLYGON` to `LINESTRING`

```{r}
geo_parks <- geo_parks %>% st_cast("MULTILINESTRING") %>% st_cast("LINESTRING")
```

Now, let us check and plot the map of the parks data.

```{r}
glimpse(geo_parks)
```

```{r}
tmap_mode("plot") +
  tm_shape(mpsz) +
  tm_polygons("REGION_N", alpha = 0.05, border.alpha = 0.05) +
  tm_shape(geo_parks) +
  tm_lines("darkgreen") +
  tm_layout(legend.position = c("right", "bottom"), 
            title= 'Parks in Singapore', 
            title.position = c('right', 'top'))
```

Great! We have successfully converted the data to `LINESTRING`!

### Prepare Good Primary Schools Dataset

[schlah.com](https://schlah.com/primary-schools) provides a good breakdown of factors that contributes to a school's ranking, based on the following extracted from their website:

-   Gifted Education Programme (GEP): 20%

-   Popularity in Primary 1 (P1) Registration: 20%

-   Special Assistance Plan (SAP): 15%

-   Singapore Youth Festival Arts Presentation: 15%

-   Singapore National School Games: 15%

-   Singapore Uniformed Groups Unit Recognition: 15%

In our analysis, we want to see if good schools can contribute to increased housing prices in Singapore. For our analysis, we will take that the top 10% (16) of primary schools in Singapore are 'good schools'

The code chunk below will extract the top 16 good primary schools for our analysis.

```{r}
TOP_10PCT_SCHS = c("NANYANG PRIMARY SCHOOL",
                  "TAO NAN SCHOOL",
                  "CATHOLIC HIGH SCHOOL",
                  "NAN HUA PRIMARY SCHOOL",
                  "ST. HILDA'S PRIMARY SCHOOL",
                  "HENRY PARK PRIMARY SCHOOL",
                  "ANGLO-CHINESE SCHOOL (PRIMARY)",
                  "RAFFLES GIRLS' PRIMARY SCHOOL",
                  "PEI HWA PRESBYTERIAN PRIMARY SCHOOL",
                  "CHIJ ST. NICHOLAS GIRLS' SCHOOL",
                  "ROSYTH SCHOOL",
                  "KONG HWA SCHOOL",
                  "POI CHING SCHOOL",
                  "HOLY INNOCENTS' PRIMARY SCHOOL",
                  "AI TONG SCHOOL",
                  "RED SWASTIKA SCHOOL")

geo_top_schools = geo_schools %>% filter(SCHOOLNAME %in% TOP_10PCT_SCHS)
glimpse(geo_top_schools)

```

There we have it, we have successfully extracted the top 10% of primary schools in Singapore (16 schools).

### Prepare CBD Outline

From [Wikipedia](https://en.wikipedia.org/wiki/Downtown_Core), we know that Singapore's CBD is also called `DOWNTOWN CORE`. To be accurate in our analysis, we will calculate the proximity to CBD based on the following rules:

-   if outside CBD boundary, we will calcualte the distance to the `LINESTRING`.

-   if within CBD, distance will be 0

The codeblock below achieves a few things:

1.  Filter to get the subzones of `DOWNTOWN CORE` planning area
2.  Combine the polygons to obtain the outline of `DOWNTOWN CORE` (CBD)
3.  Convert the geometry from `POLYGON` to `LINESTRING` format

```{r}
cbd_sf <- mpsz %>% filter(mpsz$PLN_AREA_N == "DOWNTOWN CORE")
cbd_geom <- st_union(cbd_sf)
cbd_geom <- st_cast(cbd_geom, "LINESTRING")
```

# Data Wrangling: Aspatial Data

We have three datasets that are Aspatial Data which only contains addresses of the locations. However, we cannot perform analysis without the coordinates of the datasets without its coordinates, hence, we need to geocode the data to retrieve its coordinates using onemap.

These are the datasets that require further processing:

-   CHAS Clinics

-   HDB HIP MUP

-   HDB Resale Flat Pricing

-   Shopping Malls

## Importing Aspatial Data

In the various tabs below, we will import each individual dataset from its respective folders, with a brief explanation of the use cases of each dataset.

::: panel-tabset
## CHAS Clinics

```{r}
CHAS_raw = read_xlsx("Take-Home_Ex03/aspatial/CHAS.xlsx") 
glimpse(CHAS_raw)
```

## HDB HIP MUP

```{r}
hdb_hip_mup_raw = read_xlsx("Take-Home_Ex03/aspatial/HDB_HIP-MUP-20230312.xlsx")
glimpse(hdb_hip_mup_raw)
```

## HDB Resale Flat Pricing

```{r}
hdb_resale_raw = read_csv("Take-Home_Ex03/aspatial/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv")
glimpse(hdb_resale_raw)
```

## Shopping Malls

```{r}
shopping_malls_raw = read_xlsx("Take-Home_Ex03/aspatial/malls-20230320.xlsx")
```
:::

## Filtering HDB Resale Flat Data

We will now filter the HDB Resale to focus on the target months, Jan 2021 to Feb 2023, and 5 Room HDBs to construct the predictive model. We will use:

-   *filter()* to filter out the desired room type and months

-   *unique()* to check if the desired room type and months has been filtered correctly

-   *glimpse()* to check the data structure of the filtered dataset

::: panel-tabset
## Filter Code

```{r}
hdb_resale <- filter(hdb_resale_raw, flat_type == "5 ROOM") %>%
              filter(month >= "2021-01" & month <= "2023-02")
```

## Glimpse Variables

```{r}
glimpse(hdb_resale)
```

## Unique Month and Flat_Type

```{r}
unique(hdb_resale$month)
```

```{r}
unique(hdb_resale$flat_type)
```
:::

From the code and results in the respective tabs (Glimpse Variables and Unique Month and Flat Type), we can see that:

-   There are **21,500 transactions** between Jan 2020 to Feb 2023.

-   The `month` and `flat_type` has been extracted correctly.

## Transforming Aspatial Data - Create New Columns with Values

Next, we transform the Aspatial Datasets into more meaningful values:

1.  CHAS Clinics - There is nothing to transform, since as noted earlier, there is already a `postal` column provided
2.  HDB HIP MUP - We need to obtain the address for geocoding (obtaining the SVY21 `X` and `Y` coordinates) by combining the `BLK` and `STREET` fields
3.  HDB Resale Flat Pricing - We need to obtain the address for geocoding (obtaining the SVY21 `X` and `Y` coordinates) by combining the `block` and `street_name` fields, and also convert the remaining lease from the form of `YY years MM months` to a more machine-readable format (ie. `MM` months)
4.  Shopping Malls - Nothing to transform, we can use the `Mall_Name` as the search term to obtain the geocode (SVY `X` and `Y` coordinates)

The code chunks will assist with the transformation using *mutate()*Â further explained below:

::: panel-tabset
## HDB HIP MUP

We *mutate()* the `hdb_hip_mup_raw` dataset by pasting the `BLK` and `STREET` columns together into the `address` column to a new sf dataframe called `hdb_hip_mup_trans`

```{r}
hdb_hip_mup_trans <- hdb_hip_mup_raw %>%
  mutate(hdb_hip_mup_raw, address = paste(BLK, STREET))
```

## HDB Resale Flat Pricing

We *mutate()* the `hdb_resale` dataset by pasting the `block` and `street_name` columns together into the `address` column to a new variable called `hdb_hip_mup_trans`. We also used *mutate()* to modify the existing `remaining_lease` data to the form of `MM`.

The first section of the code `as.integer(str_sub(remaining_lease, 0, 2)) * 12` extracts the year numbers as `YY` and converts it into string and then multiplying it by 12 to convert it to number of months.

The next part of the code checks if there is any numerical `MM` (month) present, if there is no month present, the value will be `NA` and 0 will be assigned in place of `NA`. Else, if present, we take the `MM`.

The integer month is summed with the year in months to form this column `remaining_lease_mths` in the new sf dataframe `hdb_resale_trans`

```{r}
hdb_resale_trans <- hdb_resale %>%
  mutate(hdb_resale, address = paste(block, street_name)) %>%
  mutate(hdb_resale, remaining_lease_mths = (as.integer(str_sub(remaining_lease, 0, 2)) * 12 + ifelse(is.na(as.integer(str_sub(remaining_lease, 9, 11))), 0,  as.integer(str_sub(remaining_lease, 9, 11)))))
```

Next, we also need to obtain the age of the HDBs. In Singapore, HDB leases are 99 years. We can take `99 - remaining_lease_mths` to obtain `age_of_unit_mths`

```{r}
hdb_resale_trans <- hdb_resale_trans %>% mutate(age_of_unit_mths = (99*12) - remaining_lease_mths)
```

Next, let us left join the HDB HIP MUP data into HDB Resale Transactions so that we know which HDB units have already completed their upgrading.

```{r}
hdb_resale_trans <- left_join(hdb_resale_trans, hdb_hip_mup_trans)
glimpse(hdb_resale_trans)
```

Then we'll select only the unnecessary columns:

```{r}
hdb_resale_trans <- hdb_resale_trans %>% select(c(1:14, 17))
```
:::

## Retrieving SVY21 Coordinate of Addresses

This section will focus on retrieving relavant data such as coordinates of the address which we could use in further spatial analysis to obtain proximity to locational factors later.

We are interested in obtaining the `SVY21` `X` and `Y` coordinates as they are in the Projected Coordinate System, which allows us to perform measure directly without any additional transformations.

### Create a List Storing Unique Addresses/Postal Codes

Since some addresses/postal codes are duplicated, we store and check unique addresses to reduce the amount of `GET` requests sent to the OneMap API:

1.  Faster
2.  OneMap API has a rate limit of 250 API calls a minute
3.  It makes it easier for us to locate errors and correct it

Here, we will obtain a list of unique addresses/postal codes for each data set.

::: panel-tabset
## CHAS Clinics

```{r}
addr_lst.chas <- sort(unique(CHAS_raw$Postal))
glimpse(addr_lst.chas)
```

## HDB Resale Flat Pricing

```{r}
addr_lst.resale <- sort(unique(hdb_resale_trans$address))
glimpse(addr_lst.resale)
```

## Shopping Malls

```{r}
addr_lst.malls <- sort(unique(shopping_malls_raw$Mall_Name))
glimpse(addr_lst.malls)
```
:::

### Create Function to Retrieve Coordinates from OneMap.sg API

The following function uses OneMap.sg Search API to obtain coordinates (SVY21 X, Y) using part of an address or postal code.

This is how the function `get_coordinates()` below will work:

1.  `new_coords` datafame is created to store all the new coordinate data and its original address that is input to the GET request API
2.  for each `addr` in `addr_lst` where `addr_lst` is the list passed into the function, we will query each record and append accordingly:
    1.  If there is 1 or more records, we append the top record's SVY21 X, Y coordinates and `addr` to a temporary dataframe called `new_row`,

    2.  Else, `NA` for it's `X` and `Y` columns and the `addr` is stored in `new_row`.
3.  The GET Request has various parameters:
    1.  `searchVal` - the value to pass to OneMap Search to obtain the Geocode (in this case we are interested in SVY21 X, Y coordinates)

    2.  `returnGeom` - return details about geometry (ie. SVY21 X, Y or Lat Lon), `Y` in this case as we want SVY21 X, Y coordinates

    3.  `getAddrDetails` - get more details about the address, `N` in this case as we don't require further information.
4.  fromJSON() helps us convert the JSON format to a list format for manipulation
    1.  the function rawToChar() was used as the received type for `reply$content`Â is RAW, which requires conversion before we can read the values
5.  Lastly, we will combine the `new_row` data into the main `new_coords` dataframe using *rbind()* as they are both dataframes.

<div>

```{r}
get_coordinates <- function(addr_lst){
  
  # Create a data frame to store all retrieved coordinates
  new_coords <- data.frame()
    
  for (addr in addr_lst){
    #print(i)

    reply <- GET('https://developers.onemap.sg/commonapi/search?',
           query = list(searchVal = addr,
                        returnGeom = 'Y',
                        getAddrDetails = 'N'))
    
    output <- fromJSON(rawToChar(reply$content))
    found <- output$found
    res <- output$results
    
    # Create a new data frame for each address
    new_row <- data.frame()
    
    # If single result, append 
    if (found >= 1){
      res_1 <- head(res, n = 1)
      x <- res_1$X
      y <- res_1$Y
      new_row <- data.frame(address = addr, x = x, y = y)
    }

    else {
      new_row <- data.frame(address = addr, x = NA, y = NA)
    }
    
    # Add the row
    new_coords <- rbind(new_coords, new_row)
  }
  return(new_coords)
}
```

</div>

### Call get_coordinates() Function to Obtain Coordinates

We use *get_coordinates()* function created earlier to obtain the coordinates of the address. *glimpse()* allows us to view and check if the data has been properly created.

RDS Scripts contains scripts to import/export the coordinates R objects to RDS file format (R Data Serialisation) prevent having to call the API each time on every render.

::: panel-tabset
## get_coordinates() Function

CHAS Clinics

```{r eval=FALSE}
coords_chas <- get_coordinates(addr_lst.chas)
```

HDB Resale Flat Pricing

```{r eval=FALSE}
coords_resale <- get_coordinates(addr_lst.resale)
```

Shopping Malls

```{r eval=FALSE}
coords_malls <- get_coordinates(addr_lst.malls)
```

## RDS Scripts

Writing RDS

```{r eval=FALSE}
write_rds(coords_chas, "Take-Home_Ex03/rds/coords_chas.rds")
write_rds(coords_resale, "Take-Home_Ex03/rds/coords_resale.rds")
write_rds(coords_malls, "Take-Home_Ex03/rds/coords_malls.rds")
```

Reading RDS

```{r}
coords_chas <- read_rds("Take-Home_Ex03/rds/coords_chas.rds")
coords_resale <- read_rds("Take-Home_Ex03/rds/coords_resale.rds")
coords_malls <- read_rds("Take-Home_Ex03/rds/coords_malls.rds")
```

## Glimpse Records

CHAS Clinics

```{r}
glimpse(coords_chas)
```

HDB Resale Flat Pricing

```{r}
glimpse(coords_resale)
```

Shopping Malls

```{r}
glimpse(coords_malls)
```
:::

## Data Verification for Coordinate Data

With the retrieved data, we need to inspect and verify the data received and correct any errors made along the way. We will do all the steps in parallel for each dataset, outlined in step format below:

1.  Merge coordinate data and original dataframe
    -   We do this as the CHAS Clinics coordinates are derived from Postal Code and it might be hard to figure out which place are we looking at by looking at just the postal code
2.  Check for `NA` X/Y values and manually amend if required
3.  Convert DataFrame into a sf Object
4.  Plot a tmap and check if points are plotted in the correct regions

At any step if there are issues, we will detail steps to fix or recover from it.

### CHAS Clinics

1.  Merge Coordinate Data and Original Dataframe

    ```{r}
    temp_chas <- left_join(CHAS_raw, coords_chas, by=c("Postal" = "address"))
    ```

2.  Check for `NA` X/Y values and manually amend if required

    ```{r}
    filter(temp_chas, is.na(x) == TRUE)
    ```

    Here, using *filter()* and *is.na()*, we find out which records do not have a valid location assigned to it. Now, let us manually check through the records and fix the issue.

    | CHAS Clinic Address                                                     | Issue                                                                        |
    |-------------------------------------------------------------------------|------------------------------------------------------------------------------|
    | 189, Selegie Road, Selegie Centre, #01- 05, Singapore 188332            | No longer exists based on Onemap and Google Map, we will remove it           |
    | 140, Corporation Drive, #01- 03                                         | Postal Code number **610140** according to OneMap, we will amend accordingly |
    | 6, Gemmill Lane                                                         | Postal Code number **069249** according to OneMap, we will amend accordingly |
    | 102, Yishun Avenue 5, #01- 133, Singapore\\r\\n760102                   | No longer exists based on Onemap and Google Map, we will remove it           |
    | 34, Craig Road, Chinatown Plaza, #01- 04,\\r\\nSingapore 089673         | No longer exists based on Onemap and Google Map, we will remove it           |
    | 1, Rochor Road, Rochor Centre, #03- 516,\\r\\nSingapore 180001          | No longer exists based on Onemap and Google Map, we will remove it           |
    | 51, TAMPINES AVENUE 4, OUR TAMPINES\\r\\nHUB, #B1- 04/05                | Records are appended as **528523** on OneMap, we will amend accordingly      |
    | 50, Market Street, Golden Shoe Car Park,\\r\\n#01- 30, Singapore 048940 | No longer exists based on OneMap and Google Map, we will remove it           |

    Now, let us update:

    #### Fixing Data

    ::: panel-tabset
    ## 1. Update Records

    We remove the clinics that are non-existent using *filter()*

    ```{r}
    chas_updated <- filter(CHAS_raw, !Address %in%
      c("189, Selegie Road, Selegie Centre, #01- 05,\r\nSingapore 188332",
        "102, Yishun Avenue 5, #01- 133, Singapore\r\n760102",
        "34, Craig Road, Chinatown Plaza, #01- 04,\r\nSingapore 089673",
        "1, Rochor Road, Rochor Centre, #03- 516,\r\nSingapore 180001",
        "50, Market Street, Golden Shoe Car Park,\r\n#01- 30, Singapore 048940"))
    ```

    Next, we use *mutate()*Â and *ifelse()* condition to update the Postal Codes of the clinics at the relavant addresses.

    ```{r}
    chas_updated <- chas_updated %>% 
      mutate(Postal = ifelse(Address == "140, Corporation Drive, #01- 03", "610140", Postal)) %>%
      mutate(Postal = ifelse(Address == "6, Gemmill Lane", "069249", Postal)) %>%
      mutate(Postal = ifelse(Address == "51, TAMPINES AVENUE 4, OUR TAMPINES\r\nHUB, #B1- 04/05", "528523", Postal))
    ```

    Lastly, we regenerate the list of unique Postal Codes to be geocoded.

    ```{r}
    addr_lst.chas_upd <- sort(unique(chas_updated$Postal))
    glimpse(addr_lst.chas_upd)
    ```

    ## 2. Rerun get_coordinates()

    We get the SVY21 X,Y coordinates using our *get_coordinates()* function

    ```{r eval=FALSE}
    coords_chas_upd <- get_coordinates(addr_lst.chas_upd)
    ```

    Saving the DataFrame as .rds for future use to prevent rerunning *get_coordinates()* GET API everytime a render is run

    ```{r eval=FALSE}
    write_rds(coords_chas_upd, "Take-Home_Ex03/rds/coords_chas_upd.rds")

    ```

    Load the DataFrame from .rds

    ```{r}
    coords_chas_upd <- read_rds("Take-Home_Ex03/rds/coords_chas_upd.rds")

    ```

    ## 3. Combine DataFrame and verify

    We left join the `chas_updated` main table and coordinates and filter the `x` column for any null values

    ```{r}
    temp_chas <- left_join(chas_updated, coords_chas_upd, by=c("Postal" = "address"))
    filter(temp_chas, is.na(x) == TRUE)
    ```

    No null values found, we have completed this step!
    :::

3.  Convert a DataFrame into a sf Object

    We specify the SVY21 X and Y coordinates to be used as the coordinate geometry. The `crs` specified is `3414` which refers to `SVY21`.

    ```{r}
    chas_sf <- st_as_sf(temp_chas,
                            coords = c("x", "y"),
                            crs = 3414)
    ```

4.  Plot a tmap and check if points are plotted in the correct regions

    Now, we will plot an interactive tmap to check if our points are correct.

    ```{r}
    tmap_mode("view")
    tm_shape(chas_sf) +
      tm_dots("Type",
              popup.vars=c("Name"="Name", "Address"="Address", "Type" = "Type", "Telephone" = "Telephone"))
    ```

    From our analysis, the points looks to be correctly located.

### HDB Resale Flat Pricing

1.  Merge Coordinate Data and Original Dataframe

    ```{r}
    temp_hdb_resale_trans <- left_join(hdb_resale_trans, coords_resale, by=c("address" = "address"))
    ```

2.  Check for `NA` X/Y values and manually amend if required

    ```{r}
    filter(temp_hdb_resale_trans, is.na(x) == TRUE)
    ```

    No `NA` values, great!

3.  Convert a DataFrame into a sf Object

    We specify the SVY21 X and Y coordinates to be used as the coordinate geometry. The `crs` specified is `3414` which refers to `SVY21`.

    ```{r}
    hdb_resale_sf <- st_as_sf(temp_hdb_resale_trans,
                            coords = c("x", "y"),
                            crs = 3414)
    ```

4.  Plot a tmap and check if points are plotted in the correct regions

    Now, we will plot an interactive tmap to check if our points are correct. We overlay the URA Master Plan Regions for a quick overlay to roughly check if the HDBs are located in the correct areas. Do note that HDB Towns differ from URA Planning Areas.

    Generate external interactive plot

    ```{r eval=FALSE}
    tmap_mode("plot")
    hdb_plot1 <- tm_shape(mpsz) +
      tm_polygons("REGION_N",
                  alpha = 0.5) +
    tm_shape(hdb_resale_sf) +
      tm_dots("town",
              popup.vars=c("block"="block", "street_name"="street_name", "flat_model" = "flat_model", "town" = "town", "resale_price" = "resale_price", "remaining_lease_mths", "remaining_lease_mths"))
    tmap_save(hdb_plot1, "thex03_hdbplot1.html")
    ```

    Static Plot

    ```{r}
    tmap_mode("plot")
    tm_shape(mpsz) +
      tm_polygons("REGION_N",
                  alpha = 0.5) +
    tm_shape(hdb_resale_sf) +
      tm_dots("town", size = 0.02)
    ```

    [View Interactive Version of Map here](thex03_hdbplot1.html){target="_blank"}! *\[20+mb\]*

    Oddly, 27 Marine Cres appeared as a point on Sembcorp Marine Tuas Crescent and 54 Kent Rd somehow appeared as a point on 54J SOUTH BUONA VISTA ROAD KENT RIDGE HILL RESIDENCES. There are also some other differences, so let us now recode some of the addresses to get them to the right locations:

    #### Fixing Data

    ::: panel-tabset
    ## 1. Update Records

    We use *mutate()* to replace the existing addresses with more specific ones that we found on OneMap.

    ```{r}
    mod_hdb_resale_trans <- hdb_resale_trans %>% 
      mutate(address = ifelse(address == "10 JLN BATU", "10 JALAN BATU DI TANJONG RHU", address)) %>%
      mutate(address = ifelse(address == "11 JLN BATU", "11 JALAN BATU DI TANJONG RHU", address)) %>%     
      mutate(address = ifelse(address == "54 KENT RD", "54 KENT ROAD KENT VILLE", address)) %>%    
      mutate(address = ifelse(address == "27 MARINE CRES", "27 MARINE CRESCENT MARINE CRESCENT VILLE", address))
    ```

    ```{r}
    temp_hdb_resale_trans <- left_join(mod_hdb_resale_trans, coords_resale, by=c("address" = "address"))
    ```

    Lastly, we regenerate the list of unique Postal Codes to be geocoded.

    ```{r}
    addr_lst.resale_upd <- sort(unique(mod_hdb_resale_trans$address))
    glimpse(addr_lst.resale_upd)
    ```

    ## 2. Rerun get_coordinates()

    We get the SVY21 X,Y coordinates using our *get_coordinates()* function

    ```{r eval=FALSE}
    coords_resale_upd <- get_coordinates(addr_lst.resale_upd)
    ```

    Saving the DataFrame as .rds for future use to prevent rerunning *get_coordinates()* GET API everytime a render is run

    ```{r eval=FALSE}
    write_rds(coords_resale_upd, "Take-Home_Ex03/rds/coords_resale_upd.rds")

    ```

    Load the DataFrame from .rds

    ```{r}
    coords_resale_upd <- read_rds("Take-Home_Ex03/rds/coords_resale_upd.rds")

    ```

    ## 3. Combine DataFrame and verify

    We left join the `hdb_hip_mup_trans_upd` main table and coordinates and filter the `x` column for any null values

    ```{r}
    temp_hdb_resale_trans <- left_join(mod_hdb_resale_trans, coords_resale_upd, by=c("address" = "address"))
    filter(temp_hdb_resale_trans, is.na(x) == TRUE)
    ```

    No null values found, we have completed this step!

    ## 4. Convert a DataFrame into a sf Object

    ```{r}
    hdb_resale_sf <- st_as_sf(temp_hdb_resale_trans,
                            coords = c("x", "y"),
                            crs = 3414)
    ```

    ## 5. Plot a tmap and check if points are plotted in the correct regions

    Generate external interactive map

    ```{r eval=FALSE}
    tmap_mode("plot")
    hdb_plot2 <- tm_shape(mpsz) +
      tm_polygons("REGION_N",
                  alpha = 0.5) +
    tm_shape(hdb_resale_sf) +
      tm_dots("town",
              popup.vars=c("block"="block", "street_name"="street_name", "flat_model" = "flat_model", "town" = "town", "resale_price" = "resale_price", "remaining_lease_mths", "remaining_lease_mths"))
    tmap_save(hdb_plot2, "thex03_hdbplot2.html")
    ```

    Static Plot

    ```{r}
    tmap_mode("plot")
    tm_shape(mpsz) +
      tm_polygons("REGION_N",
                  alpha = 0.5) +
    tm_shape(hdb_resale_sf) +
      tm_dots("town", size = 0.05)
    ```

    [View Interactive Version of Map here](thex03_hdbplot2.html){target="_blank"}! *\[20+mb\]*

    Great! All the blocks looks to be plotted in the correct locations!
    :::

### Shopping Malls

1.  Merge Coordinate Data and Original Dataframe

    ```{r}
    temp_malls <- left_join(shopping_malls_raw, coords_malls, by=c("Mall_Name" = "address"))
    ```

2.  Check for `NA` X/Y values and manually amend if required

    ```{r}
    filter(temp_malls, is.na(x) == TRUE)
    ```

    No `NA` values, great!

3.  Convert a DataFrame into a sf Object

    We specify the SVY21 X and Y coordinates to be used as the coordinate geometry. The `crs` specified is `3414` which refers to `SVY21`.

    ```{r}
    geo_malls <- st_as_sf(temp_malls,
                            coords = c("x", "y"),
                            crs = 3414)
    ```

4.  Plot a tmap and check if points are plotted in the correct regions

    Now, we will plot an interactive tmap to check if our points are correct. We overlay the URA Master Plan Regions for a quick overlay to roughly check if the malls are located in the correct areas. Do note that mall region may differ from URA Planning Areas.

    ```{r}
    tmap_mode("view")
    tm_shape(mpsz) +
      tm_polygons("REGION_N",
                  alpha = 0.5) +
    tm_shape(geo_malls) +
      tm_dots("Region",
              popup.vars=c("Mall_Name"="Mall_Name", "Region"="Region"),
              size = 0.05,
              palette = "Set2")
    ```

    Nice! All the malls seems to be in their right locations.

# Preparing Locational Factors

From our list of locational factors, we can see that in general, we have two types of locational factors:

1.  Count of a factor within a certain radius
2.  Proximity of housing to a factor

We have created functions below that will prepare out data we require for our analysis.

## Functions

### Get Proximity Locational Factors

The `get_prox()` function below takes in an origin and destination dataframe and creates a distance matrix of origin and destination pairs based on *st_distance().* Next, we use mutate and apply to locate the destination that is located the minimum distance away from the origin and save it to the corresponding row in `origin_df` under the `PROX` column. The `1`Â in apply is to apply the function row by row, which corresponds directly to the `origin_df` rows.

Next, we rename the columns based on whats specified by the input parameter and return the dataframe.

For the code below, we will use it to detect for 2 types of spatial types for destinations, Points and Linestring.

For our two linestring datasets, we know that there are:

1.  No HDBs within park boundaries
2.  No HDBs within Downtown Core

Hence, the following datasets will only need to factor distance to the boundary and there are no concerns that there are HDBs within those regions.

```{r}
get_prox <- function(origin_df, dest_df, col_name){
  
  # creates a matrix of distances
  dist_matrix <- st_distance(origin_df, dest_df)           
  
  # find the nearest location_factor and create new data frame
  near <- origin_df %>% 
    mutate(PROX = apply(dist_matrix, 1, function(x) min(x)) / 1000) 
  
  # rename column name according to input parameter
  names(near)[names(near) == 'PROX'] <- col_name

  # Return df
  return(near)
}
```

### Get Num Within Locational Factors

The `get_within()` function below takes in an origin and destination dataframe and creates a distance matrix of origin and destination pairs based on *st_distance().*

Next, we use mutate and apply to obtain the sum of destinations that fits less than or equal to the `threshold_dist` specified and save the sum value to the corresponding row in `origin_df` under the `PROX` column. The `1`Â in apply is to apply the function row by row, which corresponds directly to the `origin_df` rows.

Next, we rename the columns based on whats specified by the input parameter and return the dataframe.

```{r}
get_within <- function(origin_df, dest_df, threshold_dist, col_name){
  
  # creates a matrix of distances
  dist_matrix <- st_distance(origin_df, dest_df)   
  
  # count the number of location_factors within threshold_dist and create new data frame
  wdist <- origin_df %>% 
    mutate(WITHIN_DT = apply(dist_matrix, 1, function(x) sum(x <= threshold_dist)))
  
  # rename column name according to input parameter
  names(wdist)[names(wdist) == 'WITHIN_DT'] <- col_name

  # Return df
  return(wdist)
}
```

## Generating Locational Factors and Saving Results

::: panel-tabset
## Generating Proximity Locational Factors

Using the code chunk below, we will generate the proximity to locational factors specified below:

```{r eval=FALSE}
geo_hdb_resale <- hdb_resale_sf
geo_hdb_resale <- get_prox(geo_hdb_resale, cbd_geom, "PROX_CBD")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_eldercare, "PROX_ELDER")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_hawker, "PROX_HAWKER")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_mrt_existing, "PROX_MRT_E")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_parks, "PROX_PARK")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_top_schools, "PROX_TOP_SCH")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_malls, "PROX_MALL")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_supermarkets, "PROX_SUPMKT")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_rail_mrt_above, "PROX_TRK_MRT")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_rail_lrt_above, "PROX_TRK_LRT")
```

## Generating Num Within Locational Factors

Now, using the code chunk below, we will obtain the count of location factors specified:

```{r eval=FALSE}
geo_hdb_resale <- get_within(geo_hdb_resale, geo_childcare, 350, "NUM_350_CHILD")
geo_hdb_resale <- get_within(geo_hdb_resale, geo_kindergartens, 350, "NUM_350_KINDER")
geo_hdb_resale <- get_within(geo_hdb_resale, geo_bus_stop, 350, "NUM_350_BUS")
geo_hdb_resale <- get_within(geo_hdb_resale, geo_schools, 1000, "NUM_1000_SCH")
geo_hdb_resale <- get_within(geo_hdb_resale, geo_mrt_future, 800, "NUM_800_MRT_F")
geo_hdb_resale <- get_within(geo_hdb_resale, geo_lrt, 350, "NUM_350_LRT")
```

## RDS Scripts

To prevent running the code above again on every render, we will save the results to a RDS file. We will use the read script to read the file without having to rerun the processing on every render.

```{r eval=FALSE}
write_rds(geo_hdb_resale, "Take-Home_Ex03/rds/geo_hdb_resale.rds")
```

```{r}
geo_hdb_resale <- read_rds("Take-Home_Ex03/rds/geo_hdb_resale.rds")
```
:::

# Preparing Structural Factors

Looking at the list of structural factors, there are some factors that requires further processing. These strucutral factors are listed in the table below.

| dName of Structural Factor             | Data Type   | Remarks                                                                                                                         |
|----------------------------------------|-------------|---------------------------------------------------------------------------------------------------------------------------------|
| Area of Unit                           | Numerical   |                                                                                                                                 |
| Floor Level                            | Categorical | Requires recoding of values                                                                                                     |
| Remaining Lease                        | Numerical   | Data has been processed to numerical readable values in months in [Transforming Aspatial Data - Create New Columns with Values] |
| Age of the unit                        | Numerical   |                                                                                                                                 |
| Main Upgrading Program (MUP) Completed | Categorical | Requires one-hot encoding                                                                                                       |
| Apartment Model                        | Categorical | This data has to be derived and standardised from `flat_model`                                                                  |
| Apartment Multi-story                  | Categorical | This data has to be derived and standardised from `flat_model`                                                                  |

Now, let us first look at the floor levels.

```{r}
storeys <- sort(unique(geo_hdb_resale$storey_range))
storeys
```

From the unique values obtained above, we can see that story range is provided as a categorical range of every three floors. In the data above, we can see that there are 17 storey range categories.

Let us recode the categorical naming to numerical values by assigning 1 to the first range `01 TO 03` and 17 to the last range `49 TO 51`.

```{r}
storey_order <- 1:length(storeys)
storey_range_order <- data.frame(storeys, storey_order)
storey_range_order
```

From our data frame above, we have obtained the storey ranges and `storey_order`. Using the code below, we will use left_join to join `storey_order` to the main `geo_hdb_resale` dataframe.

```{r}
geo_hdb_resale <- left_join(geo_hdb_resale,  storey_range_order, by = c("storey_range" = "storeys"))
```

There we go, we have combined the recorded storey range values as `storey_order`.

## HDB Apartment Model and Multi-storey

Not all HDB Apartments are built the same, there are different HDB Models and some HDB units are multi-storey. Let us explore what kinds of models do we have in our dataset:

```{r}
unique(geo_hdb_resale$flat_model)
```

From our data above, we can see that we have 11 distinct categories of HDB Apartment Types. Some of these terminologies changed over time and may refer to the configuration or whether the apartments came with furnishings.

Let use understand what some of the terms [mean](https://www.teoalida.com/singapore/hdbflattypes/):

-   Design Build Sell Scheme (DBSS) flats may call for a higher value than regular HDB flats as they are designed, build and sold by 3rd party developers although they are still HDB Flats. They are supposed to be better than premium flats

-   Premium flats which come with pre-installed fittings and furnishings over standard apartments which comes with none

-   Standard flats are opposite of premium, they don't come with furnishings or fittings

-   Maisonette / Loft comes with a second floor of apartment space

-   3Gen is a new type of single-key unit with additional bedroom and bathroom for grandparents to live in

-   Adjoined flats are units where two HDB units are combined (may or may not have 2 front doors)

-   Type S2 are types assigned to 5-room units in The Pinnacle at Duxton

Let us recode them so that the model can generalise better.

::: panel-tabset
## Recode Values

| Original Values        | Recoded Values (model\_\<valuename\>) | Recoded Values (multistorey) |
|------------------------|---------------------------------------|------------------------------|
| Improved               | Standard = 1                          | 0                            |
| Standard               | Standard = 1                          | 0                            |
| DBSS                   | DBSS = 1                              | 0                            |
| Model A                | Standard = 1                          | 0                            |
| Adjoined flat          | Adjoined = 1                          | 0                            |
| Premium Apartment      | Premium = 1                           | 0                            |
| Type S2                | S2 = 1                                | 0                            |
| Model A-Maisonette     | Standard = 1                          | 1                            |
| Premium Apartment Loft | Premium = 1                           | 1                            |
| Improved - Maisonette  | Standard = 1                          | 1                            |
| 3Gen                   | 3Gen = 1                              | 0                            |

\## Create Multistorey Columns

Using the code chunk below, we will check if the `flat_model` corresponds to the following types, if it is, we code the value in the `multistorey` as 1. Else, 0 is assigned.

```{r}
geo_hdb_resale <- geo_hdb_resale %>% mutate(multistorey = ifelse(flat_model %in% c("Improved-Maisonette", "Model A-Maisonette", "Premium Apartment Loft"), 1, 0 ))
```

## Recode

```{r}
geo_hdb_resale <- geo_hdb_resale %>% mutate(model_standard = ifelse(flat_model %in% c("Improved", "Standard", "Model A", "Model A-Maisonette", "Improved-Maisonette"), 1, 0))
geo_hdb_resale <- geo_hdb_resale %>% mutate(model_premium = ifelse(flat_model %in% c("Premium Apartment", "Premium Apartment Loft"), 1, 0))
geo_hdb_resale <- geo_hdb_resale %>% mutate(model_dbss = ifelse(flat_model %in% c("DBSS"), 1, 0))
geo_hdb_resale <- geo_hdb_resale %>% mutate(model_adjoined = ifelse(flat_model %in% c("Adjoined flat"), 1, 0))
geo_hdb_resale <- geo_hdb_resale %>% mutate(model_3gen = ifelse(flat_model %in% c("3Gen"), 1, 0))
geo_hdb_resale <- geo_hdb_resale %>% mutate(model_s2 = ifelse(flat_model %in% c("Type S2"), 1, 0))
```

Nice, we have completed our recoding, let us view a snippet of our data

## Glimpse

```{r}
glimpse(geo_hdb_resale)
```
:::

## HDB HIP MUP

Similarly for HIP MUP data, since there are all coded as `HIP` or `MUP` or NA cateogrical values, we need to convert them to numbers so that the model will be able to build.

In this case, we will create two new columns, `HIP` and `MUP` to track which kind of upgrading project has been done on the unit.

::: panel-tabset
## Recode

Using the code chunk, we will recode the respective values into the respective columns as 1 (true) or 0 (false).

```{r}
geo_hdb_resale <- geo_hdb_resale %>% mutate(hip = ifelse(is.na(TYPE), 0, ifelse(TYPE == "HIP", 1, 0)))
geo_hdb_resale <- geo_hdb_resale %>% mutate(mup = ifelse(is.na(TYPE), 0, ifelse(TYPE == "MUP", 1, 0)))
```

The manipulation is complete, let us glimpse the values.

## Glimpse

```{r}
glimpse(geo_hdb_resale)
```
:::

## RDS Scripts and Preparing for EDA

We will now save our prepared HDB Resale dataset and mpsz to a RDS file and clear all variables to free up the memory before reloading the dataset as `final_resale`.

The `rm(list=ls())` function will clear all variables

```{r eval=FALSE}
write_rds(geo_hdb_resale,"Take-Home_Ex03/rds/final_resale.rds")
write_rds(mpsz,"Take-Home_Ex03/rds/mpsz.rds")
```

```{r}
rm(list=ls())
```

```{r}
final_resale <- read_rds("Take-Home_Ex03/rds/final_resale.rds")
mpsz <- read_rds("Take-Home_Ex03/rds/mpsz.rds")
```

# Exploratory Data Analysis

Now, we can perform EDA on our prepared dataset `geo_hdb_resale` to better understand our dataset.

```{r}
glimpse(final_resale)
```

## Removing Columns Not Required For Analysis

Firstly, let us remove the columns that are not required for further analysis to save on memory space.

To do this, we use the *select()* function and indicate the columns we want to remove by having the prefix`-`

```{r}
final_resale <- select(final_resale, c(-flat_type,-storey_range,-flat_model, -lease_commence_date, -address, -TYPE, -remaining_lease))
```

Now, let us view the columns remaining.

```{r}
glimpse(final_resale)
```

## Understanding Resale Prices

Now, let us plot a histogram to understand the pricing of 5-Room resale flats between Jan 2020 to Feb 2023.

```{r}
ggplot(final_resale, aes(x=resale_price)) +
  geom_histogram(bins = 20, color = "black", fill = "lightblue")
```

From our graph above, we can see that:

-   Right-skewed distribution of resale_prices

-   Most resale HDBs are transacted near the \$500,000 range.

-   Outliers are seen where HDB prices are transacted near \$1 million or more

In this scenerio, while we can take log of `resale_price`, we will not perform the transformation to make it a normal distribution as:

1.  `resale_price` is the value to be predicted, we do not want to predict the log of `resale_price`
2.  Using the log of `resale_price` will cause it to have a high correlation with actual `resale_price`

## Understanding Structural Factors

To understand more about the structural factors regarding 5-room HDB resale units of our prepared datasets, we can plot the graphs below.

Using the code chunk below, we prepare the variables to plot according to numerical (`s_factor`) and categorical data (`s_factor_cat`).

```{r}
s_factor <- c("floor_area_sqm", "storey_order", "remaining_lease_mths", "age_of_unit_mths")
s_factor_cat <- c("mup", "hip", "model_standard", "model_premium", "model_dbss", "model_adjoined", "model_3gen", "model_s2", "multistorey")
```

The code chunk below also defines the methods to plot each graph, numerical and categorical data. It will iterate the list data in `s_factor` and `s_factor_cat` respectively to seperate plots. The graph data is saved into lists `s_factor_hist_list` and `s_factor_hist_list_cat`.

```{r}
s_factor_hist_list <- c()
for (i in 1:length(s_factor)) {
  hist_plot <- ggplot(final_resale, aes_string(x = s_factor[[i]])) +
    geom_histogram(color="black", fill = "lightblue") +
    labs(title = s_factor[[i]]) +
    theme(plot.title = element_text(size = 10),
          axis.title = element_blank())
  
  s_factor_hist_list[[i]] <- hist_plot
}

s_factor_hist_list_cat <- c()
for (i in 1:length(s_factor_cat)) {
  hist_plot <- ggplot(final_resale, aes_string(x = s_factor_cat[[i]])) +
    geom_histogram(bins = 2, color="black", fill = "lightblue") +
    labs(title = s_factor_cat[[i]]) +
    theme(plot.title = element_text(size = 10),
          axis.title = element_blank())
  
  s_factor_hist_list_cat[[i]] <- hist_plot
}
```

Lastly with the list data, we can plot them using *ggarrange()* with our desired row and columns below.

**Numerical Data:**

```{r}
ggarrange(plotlist = s_factor_hist_list,
          ncol = 2,
          nrow = 2)
```

From the numerical plots above we can see that:

-   `floor_area_sqm` is right skewed, most units has a floor area of aprox. 110sqm to 120sqm. There are outlier 5-room units with sizes of about 150sqm.

-   `store_order` is right skewed. This shows that most units sold are mostly low floor units of with the most at about `storey_order` = 3, meaning 3\*3 = 9 to 11 storeys. This is so as many older HDBs are shorter in height and only newer HDBs exceed 16 storeys. Some estates like Punggol and Sengkang also has height restrictions that mostly cap new HDBs at about 16 storeys.

-   `remaining_lease_mths` is left skewed, topping at at the rightmost part of the chart. It could be seen as most homeowners trying to sell their houses after the 5-year Minimum Occupation Period. As the house is older in length, the amount of apartment sales reduces. The next peak in sales after the minimum 5-year Minimum Occupation Period is approximately 20-Years.

-   `age_of_unit_mths` is right skewed and a inverse relationship to `remaining_lease_mths`

**Categorical Data:**

```{r}
ggarrange(plotlist = s_factor_hist_list_cat,
          ncol = 3,
          nrow = 3)
```

-   Very little flats received the Main Upgrading Programme

-   About 2000 flats received the Home Improvement Porgramme (successor to the Main Upgrading Programme)

-   Standard Flat Models are approximately the inverse of Premium Flat Models

-   There is very little DBSS Model Units

-   Adjoined, S2 (Painnacle at Duxton), 3 Generation and Multistorey resale units are extremely rare

## Understanding Locational Factors

Similarly to structural factors, we will plot the locational factors to see its trends.

```{r}
l_factor <- c("PROX_CBD", "PROX_ELDER", "PROX_HAWKER", "PROX_MRT_E",
              "PROX_PARK", "PROX_TOP_SCH", "PROX_MALL", "PROX_SUPMKT",
              "PROX_TRK_MRT", "PROX_TRK_LRT", "NUM_350_CHILD", "NUM_350_BUS", "NUM_1000_SCH", "NUM_800_MRT_F", "NUM_350_LRT")

l_factor_hist_list <- c()
for (i in 1:length(l_factor)) {
  hist_plot <- ggplot(final_resale, aes_string(x = l_factor[[i]])) +
    geom_histogram(color="black", fill = "lightblue") +
    labs(title = l_factor[[i]]) +
    theme(plot.title = element_text(size = 10),
          axis.title = element_blank())
  
  l_factor_hist_list[[i]] <- hist_plot
}

```

```{r}
ggarrange(plotlist = l_factor_hist_list,
          ncol = 2,
          nrow = 2)
```

**Proximity:**

-   Proximity to CBD is somewhat slightly left skewed, there are most houses that are about 11km away from the CBD

-   Proximity to Eldercare is very right skewed, most houses have an eldercare within 1km of their house

-   Proximity to Hawker centre is very right skewed, most houses have a hawker centre within 1km of their house

-   Proximity to the nearest existing MRT station is somewhat right skewed, with most houses having access to an MRT station within 1km. We can note the interesting trend that the peak is about 0.4km to 0.8km, meaning that the placement of MRT stations could be to obtain an optimum 0.4km to 0.8km to any housing.

-   Proximity to Parks is right skewed. Most people have access to a park in under 0.8km.

-   Proximity to good schools is somewhat slightly right skewed with multiple peaks. The highest peak being at about 2.5km and second highest peaks at 0.5km and 5km.

-   Proximity to malls is right skewed, peaking at 0.5km.

-   Proximity to supermarket is right skewed with a short tail, most houses can access a supermarket within 0.5km and at most 1 to 1.5km (outlier).

-   Proximity to elevated MRT track is somewhat right skewed.

-   The proximity to LRT track is also somewhat right skewed.

**Number:**

-   The number of childcare centres in 350m of a household is somewhat right skewed, Most houses have about 4.

-   The number of bus stops within 350m of a home i somewhat normally distributed, with most having about 6 bus stops.

-   The number of houses having a school within 1km of their homes is somewhat normally distributed, most having 3 schools

-   The number of future MRT stops within 800m of a home is extremely right skewed, with most homes having none.

-   The number of LRT stations within 350m of a home is extremely right skewed, with most homes having none.

## Statistical Point Map

Let us explore more about the 5-room HDB resale dataset with its parameters.

With the code chunk below, we will generate external HTML Interactive Map pages which are appended with the proximity and within locational factors details in the popup bar in separate interactive maps.

```{r eval=FALSE}
#| code-fold: true
tmap_options("view")
statpt_tmap1 <- tm_shape(mpsz) +
  tm_polygons("REGION_N",
              alpha = 0.2) +
tm_shape(final_resale) +
  tm_dots("resale_price",
          popup.vars= c("month" = "month", "town" = "town", "block" = "block", "street_name" = "street_name", "floor_area_sqm" = "floor_area_sqm", "resale_price" = "resale_price", "remaining_lease_mths" = "remaining_lease_mths", "PROX_CBD" = "PROX_CBD", "PROX_ELDER" = "PROX_ELDER", "PROX_HAWKER" = "PROX_HAWKER", "PROX_MRT_E" = "PROX_MRT_E", "PROX_PARK" = "PROX_PARK", "PROX_TOP_SCH" = "PROX_TOP_SCH", "PROX_MALL" = "PROX_MALL", "PROX_SUPMKT" = "PROX_SUPMKT", "PROX_TRK_MRT" = "PROX_TRK_MRT", "PROX_TRK_LRT" = "PROX_TRK_LRT", "NUM_350_CHILD"),
          size = 0.05,
          style = "quantile")

statpt_tmap2 <- tm_shape(mpsz) +
  tm_polygons("REGION_N",
              alpha = 0.2) +
tm_shape(final_resale) +
  tm_dots("resale_price",
          popup.vars= c("month" = "month", "town" = "town", "block" = "block", "street_name" = "street_name", "floor_area_sqm" = "floor_area_sqm", "resale_price" = "resale_price", "remaining_lease_mths" = "remaining_lease_mths", "NUM_350_CHILD" = "NUM_350_CHILD", "NUM_350_KINDER" = "NUM_350_KINDER", "NUM_350_BUS" = "NUM_350_BUS", "NUM_1000_SCH" = "NUM_1000_SCH", "NUM_800_MRT_F" = "NUM_800_MRT_F", "NUM_350_LRT" = "NUM_350_LRT", "storey_order" = "storey_order", "multistorey" = "multistorey", "model_standard" = "model_standard", "model_dbss" = "model_dbss", "model_adjoined" = "model_adjoined", "model_3gen" = "model_3gen", "model_s2" = "model_s2", "hip" = "hip", "mup" = "mup"),
          size = 0.05,
          style = "quantile")

    tmap_save(statpt_tmap1, "thex03_statpt_tmap1.html")
    tmap_save(statpt_tmap2, "thex03_statpt_tmap2.html")
```

[View Interactive Version of Map with Proximity Locational Factors Here!](thex03_statpt_tmap1.html){target="_blank"} *\[39+mb\]*

[View Interactive Version of Map with Within Locational Factors Here!](thex03_statpt_tmap2.html){target="_blank"} *\[44+mb\]*

Using the code chunk below, we will generate the interactive map of resale prices using quantile style. The difference compared to the above maps are that the popup variables are not generated.

```{r}
#| column: screen-inset-shaded
#| code-fold: true
tmap_options("view")
tm_shape(mpsz) +
  tm_polygons("REGION_N",
              alpha = 0.2) +
tm_shape(final_resale) +
  tm_dots("resale_price",
          size = 0.05,
          style = "quantile")
```

## Computing Correlation Matrix

Before we create the OLS and Machine Learning Models to predict housing prices, we need to check if there is any strong correlation which suggests multicollinearity. We will drop one of the variable if there is a pair which correlation above 0.8 (very strong positive correlation) or below -0.8 (very strong negative correlation).

The code chunk below exports the correlation plot into a png so that the text is more readable and image is higher in resolution.

```{r}

final_resale_nogeom <- final_resale %>% st_drop_geometry()

png(file="Take-Home_Ex03/corr.png", res=300, width=2500, height=2000)
corrplot::corrplot(cor(final_resale_nogeom[,c(5, 7:34)]),
                   diag = FALSE,
                   order = "AOE",
                   tl.pos = "td",
                   tl.cex = 0.5,
                   number.cex = 0.4,
                   number.font = 2,
                   number.digits = 2,
                   method = "number",
                   type = "upper")
while (!is.null(dev.list()))  dev.off()
```

![](Take-Home_Ex03/corr.png)

From the correlation matrix above, we can see that:

-   `age_of_unit_mths` and `remaining_lease_mths` have perfect negative correlation since the `age_of_unit_mths` was derived using `(99 years x 12 mths) - remaining_lease_mths`. In this case, we will remove `age_of_unit_mths`

-   `model_standard` and `model_premium` has a very strong negative correlation of -0.88 since in most scenerios, if a unit is not a standard one, it is premium. We will remove the `model_premium` variable

Using the code chunk below, we will drop the two variables selected, `age_of_unit_mths` and `model_premium`.

```{r}
final_resale <- select(final_resale, c(-age_of_unit_mths, -model_premium))
```

## Preparing Model and Test Data

For our models, we want to use data from Jan 2021 to Dec 2022 to build our model and Jan to Feb 2023 to predict and test our models. The code below will help us split the dataset into two:

```{r}
resale.model <- final_resale %>% filter(!month %in% c("2023-01", "2023-02"))
resale.test <- final_resale %>% filter(month %in% c("2023-01", "2023-02"))
```

# OLS Multiple Linear Regression Model

## Building the OLS Model using olsrr

```{r}
#| code-fold: true
resale.mlr <- lm(formula = resale_price ~ floor_area_sqm + remaining_lease_mths + PROX_CBD + PROX_ELDER + PROX_HAWKER + PROX_MRT_E + PROX_PARK + PROX_TOP_SCH + PROX_MALL + PROX_SUPMKT + PROX_TRK_MRT + PROX_TRK_LRT + NUM_350_CHILD + NUM_350_BUS + NUM_1000_SCH + NUM_800_MRT_F + NUM_350_LRT + storey_order + multistorey + model_standard + model_dbss + model_adjoined + model_3gen + model_s2 + hip + mup,
                 data = resale.model)
ols_regress(resale.mlr)
```

Based on the results above, we can see that the P-Value for all except `model_adjoined` and `model_3gen` are statistically significant, meaning that it's P-value (`Sig`) is more than or equal to 0.05. Hence, we cannot statistically conclude that the predictor is significant in contributing towards the model and we can safely remove them.

```{r}
#| code-fold: true
resale.mlr <- lm(formula = resale_price ~ floor_area_sqm + remaining_lease_mths + PROX_CBD + PROX_ELDER + PROX_HAWKER + PROX_MRT_E + PROX_PARK + PROX_TOP_SCH + PROX_MALL + PROX_SUPMKT + PROX_TRK_MRT + PROX_TRK_LRT + NUM_350_CHILD + NUM_350_BUS + NUM_1000_SCH + NUM_800_MRT_F + NUM_350_LRT + storey_order + multistorey + model_standard + model_dbss + model_s2 + hip + mup,
                 data = resale.model)
ols_regress(resale.mlr)
```

After removing the `model_adjoined` and `model_3Gen` columns, the rest of the P-values are \< 0.05, which means all variables statistically explain the OLS Multiple Linear Regression model. Also note that the Adj. R-Squred value did not change and RMSE had only a minor change in value.

## Testing for Linear Regression Assumptions

::: panel-tabset
## Multicolinearity

```{r}
ols_vif_tol(resale.mlr)
```

From the code chunk above, we can see that there is no high Multicolinearity between the variables (VIF above 5). Hence, the Multicolinearity assumption is valid.

## Non-Linearity

```{r}
ols_plot_resid_fit(resale.mlr)
```

Next, by viewing the plot above, we can see that most values are somewhat clustered along the 0 line and are somewhat linear. Hence, we can say that the Non-Linearity assumption is valid.

## Normality

```{r}
ols_plot_resid_hist(resale.mlr)
```

The graph above plots the residuals, we can see that the residuals are normallly distributed hence, the normality assumption is valid.

## Spatial Autocorrelation

Firstly, we will convert the residual plots to Spatial Points Data Frame for further analysis later.

```{r}
resale.mlr.residuals <- as.data.frame(resale.mlr$residuals)
final_resale.mlr.residuals <- cbind(resale.model, resale.mlr.residuals) %>%
  rename(`MLR_RES` = `resale.mlr.residuals`)
final_resale.mlr.sp <- as_Spatial(final_resale.mlr.residuals)
final_resale.mlr.sp
```

Next, we plot a tmap to see the residual plots of resale units.

```{r}
tmap_mode("view")
tm_shape(mpsz) +
  tm_polygons("REGION_N",
              alpha = 0.5) +
tm_shape(final_resale.mlr.sp) +
  tm_dots("MLR_RES",
          size = 0.05)
```

From our plot above, we can see that there are signs of spatial autocorrelation. To confirm our observation is statistically true, we can perofrm the Moran's I Test. Using the code chunk below, we will calculate in sequence:

1.  Obtain the distance-based weight matrixusing *dnearneigh()* using spdep
2.  Convert neighbour lists (nb) into spatial weights
3.  Perofrm the Moran's I Test

```{r}
nb <- dnearneigh(coordinates(final_resale.mlr.sp), 0, 1500, longlat = FALSE)
nb_lw <- nb2listw(nb, style = 'W')
lm.morantest(resale.mlr, nb_lw)
```

From the results in the code chunk above:

1.  Since the P-value is \< 0.05, we will reject the null hypothesis that the spatial points are randomly distributed.
2.  Since the Observed Moran I value is \> 0, we can conclude that the spatial points resemble cluster distribution

From here, since the points resemble cluster distribution, it might be a good idea to explore Geographically Weighted methods that takes into account the neighbouring points and clustering effect which may obtain a better result.
:::

## Predicting OLS Multiple Linear Regression Model

Using the code chunk below, we will first seperate out the test Dataframe to one that is specifically for Multiple Linear Regression before we merge the predicted results into the Data Frame.

```{r}
resale.mlrtest <- resale.test
resale.mlrtest$predict  <- predict(object = resale.mlr, newdata = resale.mlrtest)
```

Next, we calculate the error in prediction for all rows using the formula `predict` - `resale_price`. A negative value means a precition that is below the actual resale price.

```{r}
resale.mlrtest <- resale.mlrtest %>% mutate(error = resale.mlrtest$predict - resale.mlrtest$resale_price)
```

### Calculating RMSE

Using the rmse() function, we can obtain the rmse score of the test data for Jan and Feb 2023 resale data for future comparison later.

```{r}
rmse(resale.mlrtest$resale_price, 
     resale.mlrtest$predict)
```

## Visualising Interactive Mpa of Errors

Next, we can plot a interactive tmap of errors using quantile style to explore the errors.

```{r}
#| column: screen-inset-shaded
#| code-fold: true
tmap_mode("view")
tm_shape(mpsz) +
  tm_polygons("REGION_N",
              alpha = 0.2) +
tm_shape(resale.mlrtest) +
  tm_dots("error",
          size = 0.05,
          style = "quantile")
```

## Visualising the Predicted Values

Using the code below, we can plot a scatterplot of the predicted values.

```{r}
mlr.plot <- ggplot(data = resale.mlrtest,
       aes(x = predict,
           y = resale_price)) +
  geom_point()
ggplotly(mlr.plot)
```

As you can see, the predicted values are mostly scattered along an imaginary line, which is good as this shows that the model is a good predictive model. We can also mouse over to see the outlier points that do not confirm close to the imaginary line.

# Geographical Random Forest Model

## Preparing Data

To obtain the bandwidth using *bw.gwr()*Â method, we need to pass the data in as a Spatial Points Data Frame. First, we will conver the resale training data (`resale.model`) as `resale.rf.sp`

```{r}
resale.rf.sp <- as_Spatial(resale.model)
```

Next, the Geographical Random Forest Model building and predictive method requires data to be formatted in a certain way. Building the model requires coordinate data and model data to be seperated.

```{r}
resale.rf.coords <- st_coordinates(resale.model)
resale.rf <- resale.model %>% st_drop_geometry()
```

Laslt,y test data requires geometry information not as a sf DataFrame but tibble Data Frame with X Y columns. The code chunk below will format it into the necessary format.

```{r}
resale.rftest.coords <- st_coordinates(resale.test)
resale.rftest <- cbind(resale.test,resale.rftest.coords) %>% st_drop_geometry()
```

## Locating Optimum Bandwidth Selection

Next, we need to obtain the optimum bandwidth to be used for building our Geographically Weighted Random Forest model. Using the code chunk below, we can obtain it.

The code takes very long to run, hence it is not evaluated but the end result is a value that can be used in the `bw` parameter when building the model.

```{r eval=FALSE}
bw_adaptive <- bw.gwr(formula = resale_price ~ floor_area_sqm + remaining_lease_mths + PROX_CBD + PROX_ELDER + PROX_HAWKER + PROX_MRT_E + PROX_PARK + PROX_TOP_SCH + PROX_MALL + PROX_SUPMKT + PROX_TRK_MRT + PROX_TRK_LRT + NUM_350_CHILD + NUM_350_BUS + NUM_1000_SCH + NUM_800_MRT_F + NUM_350_LRT + storey_order + multistorey + model_standard + model_dbss + model_adjoined + model_3gen + model_s2 + hip + mup,
                 data = resale.rf.sp,
                  approach="CV",
                  kernel="gaussian",
                  adaptive=TRUE,
                  longlat=FALSE)
```

Similarly as above, we save the bandwidth to an rds file so that we do not need to run the code everytime on render. The data will be loaded using `read_rds` from the respective directory.

```{r eval=FALSE}
write_rds(bw_adaptive, "Take-Home_Ex03/model/bw_adaptive.rds")
```

```{r}
bw_adaptive <- read_rds("Take-Home_Ex03/model/bw_adaptive.rds")
```

## Building Random Forest Model

Utilising *grf()* of SpatialML, we can build a Geographically Weighted Random Forest model by inputting the paramter to be predicted, its predictors, dataframe (without coordinates), bandwidth, type of kernel and coordinates as a seperate dataframe.

```{r eval=FALSE}
set.seed(1234)
gwRF_adaptive <- grf(formula = resale_price ~ floor_area_sqm + remaining_lease_mths + PROX_CBD + PROX_ELDER + PROX_HAWKER + PROX_MRT_E + PROX_PARK + PROX_TOP_SCH + PROX_MALL + PROX_SUPMKT + PROX_TRK_MRT + PROX_TRK_LRT + NUM_350_CHILD + NUM_350_BUS + NUM_1000_SCH + NUM_800_MRT_F + NUM_350_LRT + storey_order + multistorey + model_standard + model_dbss + model_adjoined + model_3gen + model_s2 + hip + mup,
                     dframe = resale.rf,
                     bw = bw_adaptive,
                     kernel = "adaptive",
                     coords = resale.rf.coords)
```

Similarly, since the model takes 2 hours to run and model size is about 50GB, we will save it as and RDS file to prevent rerunning it everytime on render. We also save the results into another seperate file called gwRF_results so prevent loading the entire 50GB model when we just need to view the results and importance of variables.

```{r eval=FALSE}
write_rds(gwRF_adaptive, "Take-Home_Ex03/model/gwRF_adaptive.rds")
gwRF_results <- gwRF_adaptive[["Global.Model"]]
write_rds(gwRF_results, "Take-Home_Ex03/model/gwRF_results.rds")
gwRF_adaptive <- read_rds("Take-Home_Ex03/model/gwRF_adaptive.rds")
```

This code chunk below shows the results from the building of the GWRF model.

```{r}
gwRF_results <- read_rds("Take-Home_Ex03/model/gwRF_results.rds")
gwRF_results
```

We can also understand the importance of each parameter to the Geographically Weighted Random Forest Model.

```{r}
gwRF_results[["variable.importance"]]
```

From the above code, we can see which predictors are most important towards contributing to the final predicted resale results. The top five most important parameters are:

1.  Proximity to LRT Track (`PROX_TRK_LRT`)
2.  Floor Area in SQM (`floor_area_sqm`)
3.  Proximity to Elevated MRT Track (`PROX_MRT_E`)
4.  Proximity to Top School (`PROX_TOP_SCH`)
5.  Which storey is it (`storey_order)`

## Predicting Test Data

We can use the *predict.grf()* function to predict our test dataset of 5-room HDB resale units between Jan to Feb 2023 as prepared earlier.

```{r eval=FALSE}
gwRF_pred <- predict.grf(gwRF_adaptive, 
                           resale.rftest, 
                           x.var.name="X",
                           y.var.name="Y", 
                           local.w=1,
                           global.w=0)
```

Similarly as above, we save the predicted results as RDS to prevent rerunning the predictive model. The few code chunks below will also reload the dataset and combine the predicted results into the test dataset `resale.rftest` using *cbind()*. We will also save the data as RDS to prevent having to reload and reprocess the data.

```{r eval=FALSE}
GRF_pred <- write_rds(gwRF_pred, "Take-Home_Ex03/model/GRF_pred.rds")
```

```{r eval=FALSE}
GRF_pred <- read_rds("Take-Home_Ex03/model/GRF_pred.rds")
GRF_pred_df <- as.data.frame(GRF_pred)
```

```{r eval=FALSE}
test_data_p <- cbind(resale.rftest, GRF_pred_df)
```

```{r eval=FALSE}
write_rds(test_data_p, "Take-Home_Ex03/model/test_data_p.rds")
```

```{r}
test_data_p <- read_rds("Take-Home_Ex03/model/test_data_p.rds")
```

### Caculating RMSE

The code chunk below calculates the error using the predicted value - the actual resale pricing and merges it to another column with geometry so that we can plot it on a tmap later.

```{r}
test_data_p <-  mutate(test_data_p, error = GRF_pred - resale_price)
resale.rftest.tmap <- cbind(resale.test, test_data_p$error)
names(resale.rftest.tmap)[names(resale.rftest.tmap) == 'test_data_p.error'] <- "error"
```

Using the rmse() function, we can find out what the rmse score for the model so that we can compare it later.

```{r}
rmse(test_data_p$resale_price, 
     test_data_p$GRF_pred)
```

## Visualising the Predicted Values

Using the code below, we can plot a scatterplot of the predicted values.

```{r}
gwrf.plot <- ggplot(data = test_data_p,
       aes(x = GRF_pred,
           y = resale_price)) +
  geom_point()
ggplotly(gwrf.plot)
```

As you can see, thepredicted values are mostly scattered along an imaginary line, which is good as this shows that the model is a good predictive model. We can also mouseover to see the outlier points that do not confirm close to the imaginary line.

## Visualisation of Errors

Using the interactive tmap beloow, we can plot a map of errors to explore how the Geographically Weighted Random Forest model has improved the prediction of housing prices.

```{r}
#| column: screen-inset-shaded
#| code-fold: true
tmap_mode("view")
tm_shape(mpsz) +
  tm_polygons("REGION_N",
              alpha = 0.1) +
tm_shape(resale.rftest.tmap) +
  tm_dots("error",
          size = 0.05,
          alpha = 0.7,
          style = "quantile")
```

We will go further into the comparative analysis of OLS Multiple Linear Regression Model and Geographically Weighted Random Forest models in the next section below.

# Comparative Analysis

Now that we have completed building and predicting the the OLS Linear Regression and SpatialML's Geographically Weighted Random Forest model performance, let us do a side-by-side comparison of both model's performance.

## Parameters for Each Model

As a recap, we will include predictors used in each model. Both models predict the HDB resale prices between Jan and Feb 2023 and are trained on HDB resale data between Jan 2021 to Dec 2022.

OLS Multiple Linear Regression does not contain `model_3Gen` and `model_adjoined` parameters as based on analysis by the model, the two parameters are not statistically significant to the model and are removed.

| OLS Multiple Linear Regression                               | Geographically Weighted Random Forest                        |
|--------------------------------------------------------------|--------------------------------------------------------------|
| Floor Area SQM                                               | Floor Area SQM                                               |
| Remaining Lease (Months)                                     | Remaining Lease (Months)                                     |
| Proximity to CBD (Downtown Core Boundary - Linestring) in km | Proximity to CBD (Downtown Core Boundary - Linestring) in km |
| Proximity to Eldercare in km                                 | Proximity to Eldercare in km                                 |
| Proximity to Hawker Centre in km                             | Proximity to Hawker Centre in km                             |
| Proximity to Existing MRT Station in km                      | Proximity to Existing MRT Station in km                      |
| Proximity to Park (Boundary - Linestring) in km              | Proximity to Park (Boundary - Linestring) in km              |
| Proximity to Good Primary School in km                       | Proximity to Good Primary School in km                       |
| Proximity to Mall in km                                      | Proximity to Mall in km                                      |
| Proximity to Supermarket in km                               | Proximity to Supermarket in km                               |
| Proximity to MRT Overground Track in km                      | Proximity to MRT Overground Track in km                      |
| Proximity to LRT Overground Track in km                      | Proximity to LRT Overground Track in km                      |
| Number of Childcare in 350m Radius                           | Number of Childcare in 350m Radius                           |
| Number of Bus Stops in 350m Radius                           | Number of Bus Stops in 350m Radius                           |
| Number of Primary Schools in 1km                             | Number of Primary Schools in 1km                             |
| Number of Future MRT stations in 800m                        | Number of Future MRT stations in 800m                        |
| Number of LRT stations in 350m                               | Number of LRT stations in 350m                               |
| HDB Storey Level - Recoded Values (+1 = +3 stories)          | HDB Storey Level - Recoded Values (+1 = +3 stories)          |
| Multistorey Apartment (1 - True, 0 - False)                  | Multistorey Apartment (1 - True, 0 - False)                  |
| Standard Type Apartment (1 - True, 0 - False)                | Standard Type Apartment (1 - True, 0 - False)                |
| DBSS Type Apartment (1 - True, 0 - False)                    | DBSS Type Apartment (1 - True, 0 - False)                    |
| S2 Type Apartment (1 - True, 0 - False)                      | Adjoined Type Apartment (1 - True, 0 - False)                |
| Home Improvement Programme (1 - True, 0 - False)             | 3Gen Type Apartment (1 - True, 0 - False)                    |
| Main Upgrading Programme (1 - True, 0 - False)               | S2 Type Apartment (1 - True, 0 - False)                      |
|                                                              | Home Improvement Programme (1 - True, 0 - False)             |
|                                                              | Main Upgrading Programme (1 - True, 0 - False)               |

## General Statistics

### Root Mean Squared Error (RMSE)

To compare across different models, we typically use the `RMSE` value generated from predicting the test values using the trained data to find out how does the model perform on data it has never seen before.

The RMSE value can be derived by `sqrt((y actual - y predicted)^2)`.

|          | OLS Multiple Linear Regression | Geographically Weighted Random Forest |
|----------|--------------------------------|---------------------------------------|
| **RMSE** | 84198.8                        | 55089.5                               |

The OLS Multiple Linear Regression has a RMSE error of 84198.8, meaning, the mean error for the HDB resale prices prediction is +/- \$84198.80. Similarly for Geographically Weighted Random Forest, the mean error for HDB resale prices prediction is +/- \$55089.50.

As we can see, OLS Multiple Linear Regression has a higher RMSE score than Geographically Weighted Random Forest. Hence, the Geographically Weighted Random Forest is a better model as there are lesser deviations in error

### Box Plot

Now, we will plot a boxplot to see the distribution of errors and its outliers.

```{r}
#| code-fold: true
boxplot <- plot_ly(x = resale.mlrtest$error, type = "box", name = "OLS MLR")
boxplot <- boxplot %>% add_trace(x = resale.rftest.tmap$error, name = "GWRF")

boxplot 
```

*The boxplot is interactive, you may tap the plot to inspect the points and statistics of each boxplot.*

From our box plot, we can see that the GWRF model is better in OLS MLR model in all aspects:

1.  GWR has a less most extreme (negative and positive) outlier as compared to OLS MLR model
2.  GWR a smaller range of error between the 0th Quartile and 4th Quartile, meaning that the range of errors is lesser than OLS MLR model
3.  GWR's range of errors are closer to zero, meaning predictions are closer to actual resale price of HDBs than OLS MLR model

### Understanding RMSE by Town

To understand the RMSE improvements at town levels from the OLS MLR model to the GWRF model, we can calculate the RMSE scores at HDB town level by filter and calculating the RMSE scores using each model's predicted resale price and the actual resale price.

The code chunk below extracts unique HDB towns, loops through the town names to create a dataframe containing the HDB town name, OLS RMSE score, GWRF RMSE score and the change in RMSE score from OLS to GWR. A positive score means an improvement from OLS to GWRF (ie. the errors reduced from OLS to GWRF).

```{r}
TOWN <- unique(resale.mlrtest$town)
RMSE_TOWN <- data.frame()

for (t in TOWN){
  
  x <- filter(resale.mlrtest, resale.mlrtest$town == t)
  y <- filter(test_data_p, test_data_p$town == t)
  
  xr <- rmse(x$resale_price, x$predict)
  yr <- rmse(y$resale_price, y$GRF_pred)
  
  row <- data.frame(town = t, olsrmse = xr, gwrfrmse = yr)
  RMSE_TOWN <- rbind(RMSE_TOWN, row)
}

RMSE_TOWN <- RMSE_TOWN %>% mutate(change_rmse = RMSE_TOWN$olsrmse - RMSE_TOWN$gwrfrmse)
RMSE_TOWN[order(-RMSE_TOWN$change_rmse),]
```

From viewing the changes of RMSE in different HDB towns, we can see that the RMSE scores improved in all towns except Bedok and Hougang .

This means that for predictions of HDB resale prices in all towns except Bedok and Hougang, they became more accurate moving from OLS Multiple Linear Regression model to Geographically Weighted Random Forest model. Only Bedok's and Hougang's resale pricing predictions became worse, Bedok by -\$17129.53 in mean root mean squared errors and Hougang by -\$7604.37 in RMSE.

More investigation would have to be done to understand further about why the two towns, Bedok and Hougang, became worse at predicting HDB resale prices with the GWRF model.

## Map Comparison

### Plot of Errors

```{r}
#| column: screen-inset-shaded
#| code-fold: true
tmap_mode("view")
t1 <- tm_shape(mpsz) +
  tm_polygons("REGION_N",
              alpha = 0.1) +
tm_shape(resale.mlrtest) +
  tm_dots("error",
          size = 0.05,
          alpha = 0.7,
          breaks = c(-Inf, -200000, -100000, -75000, -50000, -25000, 0, 25000, 50000, 75000, Inf))
t2 <- tm_shape(mpsz) +
tm_polygons("REGION_N",
              alpha = 0.1) +
tm_shape(resale.rftest.tmap) +
  tm_dots("error",
          size = 0.05,
          alpha = 0.7,
          breaks = c(-Inf, -200000, -100000, -75000, -50000, -25000, 0, 25000, 50000, 75000, Inf))

tmap_arrange(t1, t2, ncol = 2, sync = TRUE)
```

## Comparative Analysis of RMSE by Towns

Let us zoom into the specific HDB towns and compare the predicted errors between OLS Multiple Linear Regression and Geographically Weighted Random Forest models.

For reference, the interactive map on the left is the results from OLS Multiple Linear Regression Model and the right is from the Geographically Weighted Random Forest.

We have chosen a list of towns from different regions of Singapore that was designed with different planning principles and built in different time periods:

1.  Jurong West - West
2.  Woodlands - North
3.  Punggol - North East
4.  Tampines - East
5.  Bukit Merah - Central

### Generate Error by Town Function

Using the function below, we can plot interactive tmaps of errors in different HDB towns by calling the `generate_error_town` with 2 spatial dataframes and the HDB Town (`AREA)` to be filtered. A side-byside tmap will be generated and movement will be synced across both maps.

```{r}
generate_error_town <- function(geo1, geo2, AREA) {
  tmap_mode("view")
  plot_map_t1 <- filter(geo1, town == AREA)
  plot_map_t2 <- filter(geo2, town == AREA)
  t1 <- 
    tm_shape(plot_map_t1) +
      tm_dots("error",
            size = 0.05,
            alpha = 0.7,
            breaks = c(-Inf, -200000, -100000, -75000, -50000, -25000, 0, 25000, 50000, 75000, Inf)) +
    tm_view(set.zoom.limits = c(14, 16))
  t2 <- 
    tm_shape(plot_map_t2) +
      tm_dots("error",
            size = 0.05,
            alpha = 0.7,
            breaks = c(-Inf, -200000, -100000, -75000, -50000, -25000, 0, 25000, 50000, 75000, Inf)) +
    tm_view(set.zoom.limits = c(14, 16))
  
  tmap_arrange(t1, t2, ncol = 2, sync = TRUE)
}
```

### Generate RMSE by Town

The code block below will generate the RMSE (Root Mean Squared Errors) for each HDB Town specified. This measure could be used to compare:

1.  Same model, across towns
2.  Different model, same dataset (town or full dataset)

This could give us a sensing of how well the model is good at predicting each area's reasale house pricings.

Sample Code for `Yishun`:

```{r}
RMSE_TOWN %>% filter(town == "YISHUN")
```

### Jurong West - West

```{r}
#| column: screen-inset-shaded
#| code-fold: true
generate_error_town(resale.mlrtest, resale.rftest.tmap, "JURONG WEST")
```

*(left): OLS Multiple Linear Regression \| (right): Geographically Weighted Random Forest*

```{r}
#| code-fold: true
RMSE_TOWN %>% filter(town == "JURONG WEST")
```

There are lesser extreme errors in the Geographically Weighted Random Forest model from the OLS Multiple Linear Regression model, the Root Mean Squared Error (RMSE) for Jurong West fell from \$63789.10 to \$44813.36 when moving from OLS to GWRF models.. Most predicted HDB resale pricing is closer to 0.

Takeaway points:

1.  Best improved cluster of HDB resale units were near Corporation Rd and Corporation Dr to the South West corner of Jurong Lake Gardens. The predicted values in the GWRF model was close to actual pricing (within +/- 25,000).
2.  GWRF could have improved the model as it learns and weighs points differently based on its Geographical locations and neighbours. It acknowledges clustering effects and adjusts the weights based on its parameter values, hence, better generalising and predicting resale pricing values.

### Woodlands - North

```{r}
#| column: screen-inset-shaded
#| code-fold: true
generate_error_town(resale.mlrtest, resale.rftest.tmap, "WOODLANDS")
```

*(left): OLS Multiple Linear Regression \| (right): Geographically Weighted Random Forest*

```{r}
#| code-fold: true
RMSE_TOWN %>% filter(town == "WOODLANDS")
```

There are lesser extreme errors in the Geographically Weighted Random Forest model from the OLS Multiple Linear Regression model, the Root Mean Squared Error (RMSE) for Woodlands fell from \$64160.62 to \$42264.54 when moving from OLS to GWRF models.. Most predicted HDB resale pricing is closer to 0.

Takeaway points:

1.  Most HDB predictions improved from -\$200,000 to -\$75,000 errors from Predicted to Actual resale values to \$-50,000 to \$0 error, going from OLS MLR model to GWRF model. This means that the GWRF model can more accurately in most instances, predict housing prices closer to its actual values.
2.  GWRF could have improved the model as it learns and weighs points differently based on its Geographical locations and neighbours. It acknowledges clustering effects and adjusts the weights based on its parameter values, hence, better generalising and predicting resale pricing values. However, the GWRF model may have worsened the predictions for certain specific HDBs, specifically the ones right beside the MRT station (Admiralty and Woodlands MRT stations).

### Punggol - North East

```{r}
#| column: screen-inset-shaded
#| code-fold: true
generate_error_town(resale.mlrtest, resale.rftest.tmap, "PUNGGOL")
```

*(left): OLS Multiple Linear Regression \| (right): Geographically Weighted Random Forest*

```{r}
#| code-fold: true
RMSE_TOWN %>% filter(town == "PUNGGOL")
```

There are lesser extreme errors in the Geographically Weighted Random Forest model from the OLS Multiple Linear Regression model, the Root Mean Squared Error (RMSE) for Punggol fell from \$102508.70 to \$41495.72 when moving from OLS to GWRF models.. Most predicted HDB resale pricing is closer to 0.

Takeaway points:

1.  Most HDB predictions improved from -\$200,000 to -\$100,000 errors from Predicted to Actual resale values to \$-25,000 to \$0 error, going from OLS MLR model to GWRF model. This means that the GWRF model can more accurately in most instances, predict housing prices closer to its actual values.

2.  GWRF has improved the model as it learns and weighs points differently based on its Geographical locations and neighbours. It acknowledges clustering effects and adjusts the weights based on its parameter values, hence, better generalising and predicting resale pricing values.\
    \
    As we see, most resale housing in Punggol were closer to the actual predicted values, meaning that there is a general possibility of increased popularity of Punggol HDB resale units that could not be captured or interpreted using a Linear Regression model without context of Geographic clustering or with the predictors.\

    We can also understand this from [Statistical Point Map] if we zoom into Punggol area where the apartment units in Waterway Eastand Matilda estates are mostly in the very high outlier region. Hence, normal Linear Regression models cannot generalise this effect which may not be very well explained by the variables in put and be better understood by understanding the effects of clustering.

    \
    In this scenerio, we can see the importance of Geographic Weighted methods if clustering is observed between points as there might be unknown correlation that cannot be seen or interpreted without the knowledge of geospatial data.

### Tampines - East

```{r}
#| column: screen-inset-shaded
#| code-fold: true
generate_error_town(resale.mlrtest, resale.rftest.tmap, "TAMPINES")
```

*(left): OLS Multiple Linear Regression \| (right): Geographically Weighted Random Forest*

```{r}
#| code-fold: true
RMSE_TOWN %>% filter(town == "TAMPINES")
```

There are lesser extreme errors

There are lesser extreme errors in the Geographically Weighted Random Forest model from the OLS Multiple Linear Regression model, the Root Mean Squared Error (RMSE) for Tampines fell from \$63875.32 to \$59416.10 when moving from OLS to GWRF models. Most predicted HDB resale pricing is closer to 0.

Takeaway points:

1.  Most HDB predictions improved from -\$75,000 to -\$50,000 errors from Predicted to Actual resale values to \$-50,000 to \$0 error, going from OLS MLR model to GWRF model. This means that the GWRF model can more accurately in most instances, predict housing prices closer to its actual values.
2.  However, we also observe more instances of over-prediction of HDB resale pricing near the edges of Tampines
3.  GWRF could have improved the model as it learns and weighs points differently based on its Geographical locations and neighbours. It acknowledges clustering effects and adjusts the weights based on its parameter values, hence, better generalising and predicting resale pricing values.

### Bukit Merah - Central

```{r}
#| column: screen-inset-shaded
#| code-fold: true
generate_error_town(resale.mlrtest, resale.rftest.tmap, "BUKIT MERAH")
```

*(left): OLS Multiple Linear Regression \| (right): Geographically Weighted Random Forest*

```{r}
#| code-fold: true
RMSE_TOWN %>% filter(town == "BUKIT MERAH")
```

There are lesser extreme errors in the Geographically Weighted Random Forest model from the OLS Multiple Linear Regression model, the Root Mean Squared Error (RMSE) for Bukit Merah fell from \$116461 to \$67249.40 when moving from OLS to GWRF models. Most predicted HDB resale pricing is closer to 0.

Takeaway points:

1.  Most HDB predictions improved from the extreme less than \$-200,000 errors errors from Predicted to Actual resale values to \$-25,000 to \$0 error, going from OLS MLR model to GWRF model. This means that the GWRF model can more accurately in most instances, predict housing prices closer to its actual values.
2.  There are a few more instances of overprediction in the GWRF model though, between \$25,000 to \$75,000 to its actual resale value.
3.  GWRF could have improved the model as it learns and weighs points differently based on its Geographical locations and neighbours. It acknowledges clustering effects and adjusts the weights based on its parameter values, hence, better generalising and predicting resale pricing values.\
    \
    In this case, Bukit Merah which is nearer to the Central Business District could have its pricing corrected more since the linear regression predictors cannot estimate the effects of proximity to CBD well enough, or, there could be unknown geographical effects seen in Punggol town where resale housing in the town could be seen as more attractive to buyers.

### Bedok - East

```{r}
#| column: screen-inset-shaded
#| code-fold: true
generate_error_town(resale.mlrtest, resale.rftest.tmap, "BEDOK")
```

*left): OLS Multiple Linear Regression \| (right): Geographically Weighted Random Forest*

```{r}
#| code-fold: true
RMSE_TOWN %>% filter(town == "BEDOK")
```

The Root Mean Squared Error (RMSE) for Bedok rose from \$63,599 to \$80728.54 when moving from OLS to GWRF models. Predicted HDB prices that were initially close to \$0 predicted worse moving from OLS to GWRF models.

Takeaway points:

1.  There are a more instances of underprediction or extreme underprediction of HDB resale prices moving from OLS to GWRF models
2.  GWRF could have worsened the model as it learns and weighs points differently based on its Geographical locations and neighbours. It acknowledges clustering effects and adjusts the weights based on its parameter values, hence, better generalising and predicting resale pricing values. We cannot conclusively tell and conclude what factors made the GWRF model predict worse than the OLS model.

# Conclusion

We would like to conclude off with some insights and lessons learnt from this exercise.

There are some predictors that may have no worked according to how we intended it to be. For example, by including the proximity data about Elevated MRT track in SIngapore, we want to see the impacts of noise to HDB units located closer to the track. However, based on our OLS Multiple Linear Regression model, this may not be the case. For every 1km away from the Elevated MRT track, housing prices drop by \$11756.273 (+/- \$553.079) from [Building the OLS Model using olsrr] earlier.

Next, from our analysis in selected towns ([Punggol - North East]), we learnt that in Punggol, the 4th most improved town in terms of RMSE , we know that factoring geographical clustering effects is very important as predictor values may not be sufficient in understanding resale prices in the area, there might be other factors such as supply demand or preference for resale apartments in the area.\
\
In the case of Punggol, the large amounts of outlier resale apartments in Punggol could not inform the Linear Regression model adequately. Geographically Weighted Random Forest model could understanding the effects of clustering and adjust the resale prices in the area.

Another reason for that failure could be the Parks data set. As seen from [Transform Parks Dataset], the way Punggol Waterway' Park's extents was highlighted only included the park region near Punggol Town Centre. However, it could be possible the housing prices along the entire Punggol Waterway stretch and the proximity of those housing cause housing prices to be increased. We could explore in future to include datasets such as Water Bodies or PCN data to improve the prediction.

On the other hand, Geographically Weighted Random Forest model could make predictions worse as seen in Hougang and Bedok ([Bedok - East]) where the generated results and its RMSE scores are worse off than using Multiple Linear Regression models. However, the results are inexplicable as ultimately, Random Forest models are black boxes and we cannot backtrack to understand which part of the algorithm or variables contributed to this bad prediction.

Additionally, the drawback of GWRF over OLS MLR is that GWRF takes very long to run and generates a model that is very large in size, making quick predictions impossible. However, given a longer term use case for more accurate predictions, GWRF can be considered with proper trial and error, tuning and exploration.

To end off, accessibility metrics could also be explored instead of distance to regions. Proximity distances may not be the most accurate measure, especially distances to faraway places such as the CBD or town centre as the same 5 kiloemtres can mean 10 minutes by train if its near a train station vs 30 minutes by bus or even 60mins if more walking is required. Accessibility metrics using network could be utilised to better measure buyer's willingness to buy a flat based on time taken or convenience to certain key Points of Interests and locations.

# Credits

[Prof Kam Tin Seong - Lesson Materials](https://is415-ay2022-23t2.netlify.app)

[Nor Aisyah - Take Home Exercise 3](https://aisyahajit2018-is415.netlify.app/posts/2021-11-07-take-home-exercise-3/?panelset26=code-chunk27#test-for-spatial-autocorrelation)

[Megan Sim - Take Home Exercise 3](https://is415-msty.netlify.app/posts/2021-10-25-take-home-exercise-3/)

<https://stackoverflow.com/questions/50775357/how-to-read-in-kml-file-properly-in-r-or-separate-out-lumped-variables-into-col>

<https://www.teoalida.com/singapore/hdbflattypes/>

Relevant documentations of R Libraries
