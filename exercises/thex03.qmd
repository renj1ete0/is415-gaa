---
title: "Take-Home Exercise 03: Predicting HDB Public Housing Resale Prices Using Geographically Weighted Methods"
description: "Predicting HDB Public Housing Resale Prices Using Geographically Weighted Methods trained using Jan 2020 to Dec 2022 and tested with Jan to Feb 2023"
author: "Teo Ren Jie"
date: "3/12/2023"
date-modified: "3/19/2023"
number-sections: true
categories: ["Take-Home Exercise"]
title-block-banner: true
image: Take-Home_Ex03/preview.png
execute:
  message: false
  warning: false
---

# Overview

## Setting the Scene

\<add context\>

bal bla we are lookign at 5 storey HDB bla bla

## Objectives

ABC

## Tasks

In this take-home exercise, you are tasked to predict HDB resale prices at the sub-market level (i.e. HDB 3-room, HDB 4-room and HDB 5-room) for the month of January and February 2023 in Singapore. The predictive models must be built by using by using conventional OLS method and GWR methods. You are also required to compare the performance of the conventional OLS method versus the geographical weighted methods.

# Getting Started

## Installing and Loading Packages

Next, pacman assists us by helping us load R packages that we require, `sf`, `tidyverse` and funModeling.

```{r}
pacman::p_load(readxl, sf, tidyverse, tmap, sfdep, gifski, httr, jsonlite, onemapsgapi, rvest)
```

The following packages assists us to accomplish the following:

-   *readxl* assists us in importing `.xlsx` aspatial data without having to convert to `.csv`

-   *sf* helps to import, manage and process vector-based geospatial data in R

-   *tidyverse* which includes *readr* to import delimited text file, *tidyr* for tidying data and *dplyr* for wrangling data

-   *tmap* provides functions to allow us to plot high quality static or interactive maps using leaflet API

-   *gifski* helps us to handle the GIF animation for tmap

## 

## Data Acquisition

The following datasets would be used to create the predictive models using conventional OLS and GWR methods for HDB Resale Prices.

+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Dataset Type | Dataset Name                            | Remarks                                                                                                                                                              | Source                                                                                                                   |
+==============+=========================================+======================================================================================================================================================================+==========================================================================================================================+
| Geospatial   | URA Master Plan 2019 Subzone Boundary   | For visualisation purposes and extract Central Area                                                                                                                  | Prof Kam                                                                                                                 |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Aspatial     | HDB Resale Flat Prices                  |                                                                                                                                                                      | [data.gov.sg](https://data.gov.sg/dataset/resale-flat-prices)                                                            |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Aspatial     | HDB MUP/HIP Status                      | Manual Web Scraping                                                                                                                                                  | [hdb.gov.sg](https://services2.hdb.gov.sg/webapp/BB33RESLSTATUS/BB33PReslStatusEnq.jsp)                                  |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Aspatial     | Shopping Malls                          | Manual web scraping                                                                                                                                                  | [wikipedia.org : List of Shopping Malls in Singapore](https://en.wikipedia.org/wiki/List_of_shopping_malls_in_Singapore) |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Childcare                               |                                                                                                                                                                      | [onemap.sg Themes](https://www.onemap.gov.sg/docs/#themes)                                                               |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Kindergartens                           |                                                                                                                                                                      | [onemap.sg Themes](https://www.onemap.gov.sg/docs/#themes)                                                               |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Eldercare                               |                                                                                                                                                                      | [onemap.sg Themes](https://www.onemap.gov.sg/docs/#themes)                                                               |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Foodcourt/Hawker                        |                                                                                                                                                                      | [onemap.sg Themes](https://www.onemap.gov.sg/docs/#themes)                                                               |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Supermarket                             |                                                                                                                                                                      | [onemap.sg](onemap.sg)                                                                                                   |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Current and Future MRT/LRT Stations     | Excludes Cross Region Line Punggol Branch                                                                                                                            | [data.gov.sg](https://data.gov.sg/dataset/master-plan-2019-rail-line-layer)                                              |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| \-           | Future MRT Station (CRL Punggol Branch) | Manually merge into MRT/LRT Station Dataset                                                                                                                          | [wikipedia.org : Elias MRT Stn](https://en.wikipedia.org/wiki/Elias_MRT_station)\                                        |
|              |                                         |                                                                                                                                                                      | [wikipedia.org : Riveria MRT Stn](https://en.wikipedia.org/wiki/Riviera_MRT/LRT_station)                                 |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | MRT/LRT Railway Line                    | Filter elevated sections of MRT line                                                                                                                                 | [data.gov.sg](https://data.gov.sg/dataset/master-plan-2019-rail-line-layer)                                              |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Bus Stops                               |                                                                                                                                                                      | [datamall.lta.gov.sg](https://datamall.lta.gov.sg/content/dam/datamall/datasets/Geospatial/BusStopLocation.zip)          |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Parks                                   | We consider the rail corridor, nature reserves and parks as parks as they are for leisure purposes.                                                                  | [data.gov.sg](https://data.gov.sg/dataset/nparks-parks-and-nature-reserves)                                              |
|              |                                         |                                                                                                                                                                      |                                                                                                                          |
|              |                                         | Also, we will prefer polygons of parks as we can calculate the actual proximity to the edges of the parks instead of to an arbitary point in the centre of the park. |                                                                                                                          |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Geospatial   | Primary Schools                         | Requires special handling                                                                                                                                            | onemap.sg json                                                                                                           |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+
| Aspatial     | CHAS Clinics                            | Extracted using Excel from PDF                                                                                                                                       | [chas.sg](https://www.chas.sg/clinic-locator)                                                                            |
+--------------+-----------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------+

## Data Fields

The data fields we are looking to incorporate and work with in our predictive models includes:

::: panel-tabset
## Structural Factors

-   Area of the unit

-   Floor level

-   Remaining lease

-   Age of the unit

-   Main Upgrading Program (MUP) completed

    -   Extracted MUP and Home Improvment Programme (HIP) data from HDB website

    -   For HDB units that has received HIP, their home value may be affected positively than a similar aged flat that has not received it

-   Flat Model (eg. DBSS/Standard/Premium)

    -   Design Build Sell Scheme (DBSS) flats may call for a higher value than regular HDB flats as they are designed, build and sold by 3rd party developers although they are still HDB Flats. They are supposed to be better than premium flats

    -   Premium flats which come with pre-installed fittings and furnishings over standard apartments which comes with none

    -   Reference: https://www.teoalida.com/singapore/hdbflattypes/

-   Flat Multi-storey (Maisonette or Loft)

    -   Some homeowners may prefer multi-story HDBs over single-story ones

## Locational Factors

-   Proximity to CBD

-   Proximity to eldercare

-   Proximity to foodcourt/hawker centres

-   Proximity to MRT

-   Proximity to park

-   Proximity to good primary school

-   Proximity to shopping mall

-   Proximity to supermarket

-   Numbers of kindergartens within 350m

-   Numbers of childcare centres within 350m

-   Numbers of bus stop within 350m

-   Numbers of primary school within 1km

-   Proximity to Overhead MRT Line \[noise concern\]

    -   The closer a HDB unit is to the MRT track, the home value might be affected due to noise concerns. We measure the proximity of HDB units using its euclidean distance to the closest part of the MRT track if it is less than 300metres away.

-   Proximity to Overhead LRT Line (similar to MRT line)

-   Number of Future MRT stops within 800m (10min walk)

    -   Here, I want to explore how the resale values of HDBs could be affected by future MRT stations that are announced but not yet built. Home owners may be enticed to buy houses near future MRT lines in hopes that the house values will increase and also due to increased connectivity

-   Number of LRT Stops within 350m

    -   The metric is necessary as LRT serves as a feeder within the town and is typically used short-haul vs MRT which is between various towns. The 350m metric is derived from Bus Stops differentiates the weight between a LRT stop and MRT stop especially if the LRT stop is far away from the MRT stop in towns such as Sengkang, Punggol and Pasir Ris
:::

# Data Wrangling: Geospatial Data

There are two categories of datasets we will need for our analysis, these includes:

-   Datasets that has been downloaded - These files are already downloaded into a local location

-   Datasets that are retrieved over API - We need to obtain the datasets using API Calls

## Importing / Retrieving / Obtaining Data

### Retrieving Data from API Calls

There are some data that we need to retreive using API calls from [onemap.sg](onemap.sg). OneMap offers additional data from different government agencies through [Themes](https://www.onemap.gov.sg/docs/#themes). For R, the [onemapsgapi](https://cran.r-project.org/web/packages/onemapsgapi/onemapsgapi.pdf) package helps us with the API calls with onemap.sg servers to obtain the data we require.

Using onemapsgapi is pretty simple as shown below:

```{r eval=FALSE}
token <- "" # enter authentication token obtained from onemap
search_themes(token, "<searchterm>") %>% print(n=Inf)
tibble <- get_theme(token, "<queryname>")
sf <- st_as_sf(tibble, coords=c("Lng", "Lat"), crs=4326)
```

-   *search_themes()* - Search for various thematic layers provided by onemap (eg. Parks). A tibble dataframe will be provided with more details of the layer, such as the `THEMENAME`, `QUERYNAME`, `ICON`, `CATEGORY` and `THEME_OWNER`

-   *get_theme()* - Using the desired theme's `QUERYNAME` obtained from *search_themes()*, we can obtain the thematic data in a tibble dataframe. We will need to use st_as_sf to specify the `Lat`, `Lng` and crs to obtain it as a sf dataframe.

Listed below are a list of layers we need to obtain:

-   Childcare

-   Kindergartens

-   Eldercare

-   Foodcourt/Hawker Centres

In the code block below, we will assume to have used *search_themes()* to pick the specific themes we want, to load them. The justification will be listed below.

::: panel-tabset
## Obtain Data

Childcare

```{r eval=FALSE}
childcare_tibble <- get_theme(token, "childcare")
childcare_sf <- st_as_sf(childcare_tibble, coords=c("Lng", "Lat"), crs=4326)
```

Kindergartens

```{r eval=FALSE}
kindergartens_tibble <- get_theme(token, "kindergartens")
kindergartens_sf <- st_as_sf(kindergartens_tibble, coords=c("Lng", "Lat"), crs=4326)
```

Eldercare

```{r eval=FALSE}
eldercare_tibble <- get_theme(token, "eldercare")
eldercare_sf <- st_as_sf(eldercare_tibble, coords=c("Lng", "Lat"), crs=4326)
```

Foodcourt/Hawker Centre

```{r eval=FALSE}
hawker_tibble <- get_theme(token, "hawkercentre_new")
hawker_sf <- st_as_sf(hawker_tibble, coords=c("Lng", "Lat"), crs=4326)
```

## RDS Scripts (Save)

```{r eval=FALSE}
write_rds(childcare_sf, "Take-Home_Ex03/rds/childcare_sf.rds")
write_rds(kindergartens_sf, "Take-Home_Ex03/rds/kindergartens_sf.rds")
write_rds(eldercare_sf, "Take-Home_Ex03/rds/eldercare_sf.rds")
write_rds(hawker_sf, "Take-Home_Ex03/rds/hawker_sf.rds")
```
:::

### Obtaining Schools Data

Obtaining school data from OneMap is a bit tricky, it was not available through OneMap themes or a download link through the OneMap website. However, through clicking through the Query Schools function on the map using using 'Inspect Element', we could see that a GET request is called to obtain the map data as json (as seen in the screenshot below):

![](Take-Home_Ex03/geospatial/onemap-schools-1.png)

By opening the link, we could see that it is an undocumented public API that OneMap uses to retrieve map data regarding Primary Schools. The results are in json as shown below:

![](Take-Home_Ex03/geospatial/onemap-schools-2.png)

The data has been downloaded and will be processed into tibble format using *json_lite* *fromJSON()* which will import the JSON file and convert it into tibble dataframe.

```{r}
schools_tibble <- fromJSON("Take-Home_Ex03/geospatial/retrieveAllSchools.json")[["SearchResults"]]
glimpse(schools_tibble)
```

As we can see the want to exclude the column `PageCount` and the first row as it is not relavant to our dataset. The code chunk below will perform the above for us:

```{r}
schools_tibble <- select(schools_tibble,-"PageCount")
schools_tibble <- schools_tibble[-1,]
```

Next, we will convert the tibble dataframe to sf dataframe. Since X and Y coordinates are provided for us (SVY21) in the columns `SCH_Y_ADDR` and `SCH_X_ADDR`, we will use them instead of the `Lng` and `Lat` as SVY21 (Projected Coordinate System) will allow us to perform our analysis directly.

```{r}
schools_sf_3414 <- st_as_sf(schools_tibble, coords=c("SCH_X_ADDR", "SCH_Y_ADDR"), crs=3414)
```

Now, we will save the data imported as RDS file format (R Data Serialisation).

```{r eval=FALSE}
write_rds(schools_sf_3414, "Take-Home_Ex03/rds/schools_sf_3414.rds")
```

### Importing Geospatial Data

::: panel-tabset
## Downloaded

Master Plan Subzone 2019

```{r}
mpsz = st_read(dsn = "Take-Home_Ex03/geospatial", layer="MPSZ-2019")
```

Current and Future MRT/LRT Stations

```{r}
geo_mrt_lrt_stn = st_read(dsn = "Take-Home_Ex03/geospatial/master-plan-2019-rail-station-layer-kml.kml")
```

MRT/LRT Railway Line

```{r}
geo_railway_line = st_read(dsn = "Take-Home_Ex03/geospatial/rail-line.kml")
```

Bus Stops

```{r}
geo_bus_stop = st_read(dsn = "Take-Home_Ex03/geospatial", layer="BusStop")
```

Parks

```{r}
geo_parks = st_read(dsn = "Take-Home_Ex03/geospatial/nparks-parks-and-nature-reserves-kml.kml")
```

Supermarket

```{r}
geo_supermarkets = st_read(dsn = "Take-Home_Ex03/geospatial", layer="Supermarkets")
```

## Other Data (RDS)

```{r}
geo_schools <- read_rds("Take-Home_Ex03/rds/schools_sf_3414.rds")
geo_childcare <- read_rds("Take-Home_Ex03/rds/childcare_sf.rds")
geo_eldercare <- read_rds("Take-Home_Ex03/rds/eldercare_sf.rds")
geo_hawker <- read_rds("Take-Home_Ex03/rds/hawker_sf.rds")
geo_kindergartens <- read_rds("Take-Home_Ex03/rds/kindergartens_sf.rds")
```
:::

## Transforming Coordinate Systems

For datasets in `WGS84` Geodetic Coordinate System, we need to convert them to `SVY21` Projected Coordinate System to perform our analysis. Inferring form the information above, we will use the code chunk below to confirm all of them.

::: panel-tabset
## Transform

We use *st_zm()* on the kml datasets to remove the Z dimensions which will cause issues with analysis later as XY and XYZ data do not work well with one another.

```{r}
mpsz <- st_transform(mpsz,3414)
geo_mrt_lrt_stn <- st_transform(st_zm(geo_mrt_lrt_stn),3414)
geo_railway_line <- st_transform(st_zm(geo_railway_line),3414)
geo_parks <- st_transform(st_zm(geo_parks),3414)
geo_supermarkets <- st_transform(geo_supermarkets,3414)
geo_childcare <- st_transform(geo_childcare,3414)
geo_eldercare <- st_transform(geo_eldercare,3414)
geo_hawker <- st_transform(geo_hawker,3414)
geo_kindergartens <- st_transform(geo_kindergartens,3414)
```

## Check

Bus Stop

```{r}
st_crs(geo_bus_stop)
```

Oh, the CRS was not set properly and reflected as EPSG:9001

```{r}
geo_bus_stop <- st_set_crs(geo_bus_stop, 3414)
st_crs(geo_bus_stop)
```

Done!

Master Plan Subzone 2019

```{r}
mpsz
```

Current and Future MRT/LRT Stations

```{r}
geo_mrt_lrt_stn
```

MRT/LRT Railway Line

```{r}
geo_railway_line
```

Parks

```{r}
geo_parks
```

Supermarkets

```{r}
geo_supermarkets
```

Childcare

```{r}
geo_childcare
```

Eldercare

```{r}
geo_eldercare
```

Hawker

```{r}
geo_hawker
```

Kindergartens

```{r}
geo_kindergartens
```
:::

Great! Now everything is in `SVY21` Projected Coordinate System.

## Transform Datasets

### Fixing Master Plan Subzone Boundary Geometries

As we recall for exercises in class, there are issues with invalid geometries in the dataset.

```{r}
length(which(st_is_valid(mpsz) == FALSE))
```

Here, we will fix it by using *st_make_valid()*

```{r}
mpsz <- st_make_valid(mpsz)
length(which(st_is_valid(mpsz) == FALSE))
```

Great, its fixed!

### Fixing KML Data

When we look at the MRT/LRT Station and Railway Line stations, we find that the labels are `KML_1`, `KML_2`, etc which are not useful for our analysis.

```{r}
glimpse(geo_mrt_lrt_stn)
```

```{r}
glimpse(geo_railway_line)
```

Here, we can see that many of the attributes are nested in a HTML format under the `description` column, We will now fix the `KML` imported data for MRT/LRT Station and Railway Line datasets so we can access the attributes to filter it effectively for our further analysis. The code referenced is from [StackOverflow](https://stackoverflow.com/questions/50775357/how-to-read-in-kml-file-properly-in-r-or-separate-out-lumped-variables-into-col):

```{r}
attributes <- lapply(X = 1:nrow(geo_mrt_lrt_stn), 
                     FUN = function(x) {

                       geo_mrt_lrt_stn %>% 
                         slice(x) %>%
                         pull(Description) %>%
                         read_html() %>%
                         html_node("table") %>%
                         html_table(header = TRUE, trim = TRUE, dec = ".", fill = TRUE) %>%
                         as_tibble(.name_repair = ~ make.names(c("Attribute", "Value"))) %>% 
                         pivot_wider(names_from = Attribute, values_from = Value)

                     })
geo_mrt_lrt_stn <- 
  geo_mrt_lrt_stn %>%
  bind_cols(bind_rows(attributes)) %>%
  select(-Description)

glimpse(geo_mrt_lrt_stn)
```

```{r}
attributes <- lapply(X = 1:nrow(geo_railway_line), 
                     FUN = function(x) {

                       geo_railway_line %>% 
                         slice(x) %>%
                         pull(Description) %>%
                         read_html() %>%
                         html_node("table") %>%
                         html_table(header = TRUE, trim = TRUE, dec = ".", fill = TRUE) %>%
                         as_tibble(.name_repair = ~ make.names(c("Attribute", "Value"))) %>% 
                         pivot_wider(names_from = Attribute, values_from = Value)

                     })
geo_railway_line <- 
  geo_railway_line %>%
  bind_cols(bind_rows(attributes)) %>%
  select(-Description)

glimpse(geo_railway_line)
```

Great now we have extracted the attributes into its own columns where we can use it for further analysis.

### Transforming and Modifying MRT/LRT Station Data

Let us view `geo_mrt_lrt_stn` data on a map and the table and fix any NA values we might find:

::: panel-tabset
## Map

```{r}
tmap_mode("plot") +
  tm_shape(mpsz) +
  tm_polygons("REGION_N", alpha = 0.1, border.alpha = 0.1) +
  tm_shape(geo_mrt_lrt_stn) +
  tm_fill("RAIL_TYPE", palette =c("red", "blue")) +
  tm_layout(legend.position = c("right", "bottom"), 
          title= 'MRT/LRT Stations in Singapore', 
          title.position = c('right', 'top'))
```

## Glimpse

```{r}
glimpse(geo_mrt_lrt_stn)
```

## Checking and fixing NA Values

Filter and view data

```{r}
geo_stn_na <- filter(geo_mrt_lrt_stn,NAME == "")
geo_stn_na
```

View on a map

```{r}
tmap_mode("view") +
  tm_shape(mpsz) +
  tm_polygons("REGION_N", alpha = 0.1, border.alpha = 0.1) +
  tm_shape(geo_stn_na) +
  tm_fill("Name", palette =c("red", "blue"), popup.vars=c("NAME" = "NAME"))
```

From the map and data above, we can see 9 stations has its names missing as shown below:

| Name (KML Name) | NAME (Station Name)          |
|-----------------|------------------------------|
| kml_74          | Imbiah (Sentosa Express)     |
| kml_75          | Beach (Sentosa Express)      |
| kml_77          | Downtown (DTL)               |
| kml_80          | Chinatown (DTL)              |
| kml_92          | Newton (DTL)                 |
| kml_97          | Maxwell (TEL)                |
| kml_107         | Waterfront (Sentosa Express) |
| kml_150         | Marina East (TEL)            |
| kml_203         | Orchard Boulevard (TEL)      |

We don't want the Sentosa Express data as it serves more for leisure purpose. We will drop it from the dataframe later.

Fixing Data

```{r}
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_74", "NAME"] <- "IMBIAH"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_75", "NAME"] <- "BEACH"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_77", "NAME"] <- "DOWNTOWN"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_80", "NAME"] <- "CHINATOWN"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_92", "NAME"] <- "NEWTON"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_97", "NAME"] <- "MAXWELL"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_107", "NAME"] <- "WATERFRONT"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_150", "NAME"] <- "MARINA SOUTH"
geo_mrt_lrt_stn[geo_mrt_lrt_stn$Name == "kml_203", "NAME"] <- "ORCHARD BOULEVARD"
```
:::

There are a few steps to obtaining the data in the format we want.

We want the data in three dataframes:

1.  Existing MRT stations - North South Line, East West Line, Changi Airport Line, North East Line, Circle Line, Downtown Line, Thomson East Coast Line 1, 2 and 3
2.  Existing LRT stations - Bukit Panjang LRT, Sengkang LRT, Punggol LRT
3.  Future MRT stations - Thomson East Coast Line 4, 5, Jurong Region Line, Cross Island Line 1, Punggol Extension (we need to manually insert the stations)

The reason why Cross Island Line 2 was not included is that it is only [announced on 20 Sep 2022](https://www.lta.gov.sg/content/ltagov/en/newsroom/2022/9/news-releases/cross-island-line-phase-2.html) which is outside of our model data range. Hence, those stations would not have affected the housing prices in any way. We also want to exclude stations that do not have a definite opening date (Bukit Brown, Marina South and Mount Pleasant).

There are also a few other hurdles we need to go through:

1.  Interchange MRT stations have multiple polygons and records, we need to merge them
2.  For our analysis, we want to convert the polygons to points to be able to perform our analysis.

### Extraction of Data into Different DataFrames

::: panel-tabset
## Future MRT

```{r}
FUTURE_MRT = c("CHOA CHU KANG WEST", "TENGAH", "TENGAH PLANTATION", "TENGAH PARK", "BUKIT BATOK WEST", "TOH GUAN", "JURONG TOWN HALL", "PANDAN RESERVOIR", "HONG KAH", "CORPORATION", "JURONG WEST", "BAHAR JUNCTION", "GEK POH", "TAWAS", "NANYANG GATEWAY", "NANYANG CRESCENT", "PENG KANG HILL", "ENTERPRISE", "TUKANG", "JURONG HILL", "JURONG PIER", "FOUNDERS' MEMORIAL", "TANJONG RHU", "KATONG PARK", "TANJONG KATONG", "MARINE PARADE", "MARINE TERRACE", "SIGLAP", "BAYSHORE", "BEDOK SOUTH", "SUNGEI BEDOK", "XILIN", "AVIATION PARK", "LOYANG", "PASIR RIS EAST", "TAMPINES NORTH", "DEFU", "SERANGOON NORTH", "TAVISTOCK", "TECK GHEE", "HUME", "KEPPEL", "CANTONMENT", "PRINCE EDWARD ROAD", "PUNGGOL COAST")

EXCLUDE = c("MARINA SOUTH", "BUKIT BROWN", "MOUNT PLEASANT", "WATERFRONT", "BEACH", "IMBIAH")

geo_mrt_future <- geo_mrt_lrt_stn %>%
  filter(NAME %in%  FUTURE_MRT)

glimpse(geo_mrt_future)
```

Looks correct! We have 44 unique future MRT stations that are new (excludes new interchanges with existing lines), 1 unique station is Sungei Bedok which is an interchange on TEL and DTL, hence, 44 records.

## Existing LRT

```{r}
geo_lrt <- geo_mrt_lrt_stn %>%
  filter(RAIL_TYPE == "LRT") %>% filter(!NAME %in% EXCLUDE)

glimpse(geo_lrt)
```

Looks correct! We have 42 LRT stations in Singapore.

## Existing MRT

```{r}
geo_mrt_existing <- geo_mrt_lrt_stn %>%
  filter(RAIL_TYPE == "MRT") %>% filter(!NAME %in% EXCLUDE) %>% filter(!NAME %in% FUTURE_MRT)
geo_mrt_existing
```

By looking through the dataframe, the data looks correct!
:::

#### Merging Polygons for Data Frame

For `geo_mrt` which contains data of existing MRT stations, there are interchange stations which has seperate polygons. For example, Dhoby Ghaut MRT station is an interchange between 3 lines and hence has 3 polygons and records as seen below:

```{r}
filter(geo_mrt_existing, NAME == "DHOBY GHAUT INTERCHANGE")
```

We want to merge the records to obtain a single spatial point for each MRT station. Below, we will identify the interchange stations and merge their records and polgons manually.

::: panel-tabset
## Merge Functions

Function to merge 2 and 3 rows respectively

```{r}
merge_2 <- function(df, kml_1, kml_2){
  operation <- st_union(filter(df,  Name == kml_1), filter(df,  Name == kml_2))
  operation <- select(operation, "geometry")
  df[df$Name == kml_1, "geometry"] <- operation
  df <- subset(df, Name != kml_2)
  
  return(df)
}

merge_3 <- function(df, kml_1, kml_2, kml_3){
  operation <- st_union(filter(df,  Name == kml_1), filter(df,  Name == kml_2))
  operation <- select(operation, c(0:6, "geometry"))
  operation <- st_union(operation, filter(df,  Name == kml_3))
  operation <- select(operation, "geometry")
  df[df$Name == kml_1, "geometry"] <- operation
  df <- subset(df, Name != kml_2)
  df <- subset(df, Name != kml_3)
  
  return(df)
}
```

## Merging Polygons

The polygons are merged for the stations as indicated in the code block

```{r}
# ANG MO KIO
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_20", "kml_236")
# BISHAN
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_43", "kml_247")
# BOON LAY
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_180", "kml_205")
# BOTANIC GARDENS
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_210", "kml_211")
# BONUA VISTA
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_227", "kml_228")
# CALDECOTT
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_231", "kml_232")
# CHINATOWN
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_80", "kml_165")
# CHOA CHU KANG
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_118", "kml_187")
# DHOBY GHAUT
geo_mrt_existing <- merge_3(geo_mrt_existing, "kml_156", "kml_157", "kml_158")
# EXPO
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_108", "kml_174")
# HARBOURFRONT
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_58", "kml_59")
# HOUGANG
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_11", "kml_245")
# JURONG EAST
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_135", "kml_136")
# LITTLE INDIA
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_160", "kml_161")
# MACPHERSON
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_222", "kml_223")
# MARINA BAY
geo_mrt_existing <- merge_3(geo_mrt_existing, "kml_68", "kml_78", "kml_147")
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_68", "kml_148")
# NEWTON
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_72", "kml_92")
# ORCHARD
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_98", "kml_154")
# OUTRAM PARK
geo_mrt_existing <- merge_3(geo_mrt_existing, "kml_100", "kml_151", "kml_251")
# PASIR RIS
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_239", "kml_243")
# PAYA LEBAR
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_153", "kml_212")
# SERANGOON
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_8", "kml_10")
# STEVENS
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_105", "kml_209")
# TAMPINES
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_35", "kml_166")
# WOODLANDS
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_64", "kml_169")
# BUGIS
geo_mrt_existing <- merge_2(geo_mrt_existing, "kml_81", "kml_70")
```

That's right, we have 134 unique existing MRT stations
:::

#### Converting Spatial Polygons to Spatial Points

```{r}
geo_mrt_existing <- st_centroid(geo_mrt_existing)
geo_lrt <- st_centroid(geo_lrt)
geo_mrt_future <- st_centroid(geo_mrt_future)
```

#### Insert Cross Island Line Punggol Future Stations

The Master Plan 2019 MRT/LRT Station data excludes the Cross Island Line Punggol Stations, so we have to add them. 2 New MRT stations (that are not an existing interchange station with existing lines needs to be added). These are: [Riveria](https://en.wikipedia.org/wiki/Riviera_MRT/LRT_station)and [Elias](https://en.wikipedia.org/wiki/Elias_MRT_station)

```{r}
new_df <- data.frame(Name = "kml_998", GRND_LEVEL = "UNDERGROUND", RAIL_TYPE = "MRT", NAME = "ELIAS", INC_CRC = "", FMEL_UPD_D = "", lng = "103.984", lat = "1.384")
new_df_coords <- st_as_sf(new_df, coords = c("lng", "lat"), crs=4326) 
new_df_coords <- new_df_coords %>% st_transform(3414)
geo_mrt_future <- rbind(new_df_coords, geo_mrt_future)

new_df <- data.frame(Name = "kml_999", GRND_LEVEL = "UNDERGROUND", RAIL_TYPE = "MRT", NAME = "RIVERIA", INC_CRC = "", FMEL_UPD_D = "", lng = "103.916772", lat = "1.394439")
new_df_coords <- st_as_sf(new_df, coords = c("lng", "lat"), crs=4326) 
new_df_coords <- new_df_coords %>% st_transform(3414)
geo_mrt_future <- rbind(new_df_coords, geo_mrt_future)
```

### Verifying MRT/LRT Data

```{r}
tmap_mode("plot") +
  tm_shape(mpsz) +
  tm_polygons("REGION_N", alpha = 0.05, border.alpha = 0.05) +
  tm_shape(geo_mrt_existing) +
  tm_dots("RAIL_TYPE", palette = "darkgreen", title = "Existing MRT", size = 0.02) +
  tm_shape(geo_lrt) +
  tm_dots("RAIL_TYPE", palette = "blue", title = "Existing LRT", size = 0.02) +
  tm_shape(geo_mrt_future) +
  tm_dots("RAIL_TYPE", palette = "red", title = "Future MRT", size = 0.02) +
  tm_layout(legend.position = c("right", "bottom"), 
            title= 'MRT/LRT Stations in Singapore', 
            title.position = c('right', 'top'))
```

Everything looks to be plotted correctly.

### Transforming Railway Line

```{r}
tmap_mode("plot") +
  tm_shape(mpsz) +
  tm_polygons("REGION_N", alpha = 0.05, border.alpha = 0.05) +
  tm_shape(geo_railway_line) +
  tm_lines(c("GRND_LEVEL", "RAIL_TYPE"), palette = c("red", "blue", "darkgreen")) +
  tm_layout(legend.position = c("right", "bottom"), 
            title= 'Railway Line in Singapore', 
            title.position = c('right', 'top'))
```

As we can see from our tmap plot above, the dataset contains:

1.  `GRND_LEVEL` - Whether the track segment is above or underground
2.  `RAIL_TYPE` - Whether the track belongs to `LRT`, `MRT` or `RAILWAY` (KTM train)

Do note that since the data is extracted from URA Master Plan 2019 Rail Line, we will be able to see all current and future rail lines (Thomson East Coast Lines Stages 4, 5, Cross Island Line 1, Jurong Region Line).

For our analysis, we only want the above ground segments, seperated by `RAIL_TYPE` but excluding KTM data, as generally above ground segments affects residents the most. The reason why we seperate it by `RAIL_TYPE` is that LRT makes lesser noise than MRT and may not adversely impact housing prices as much as MRT. For MRTs, [NUS researchers](https://ireus.nus.edu.sg/mrt-and-property-value/) found that housing values were impacted by noise.

The rationale of including future aboveground lines like the Jurong Region Line in our analysis is that housing prices could be affected by the construction or announcement of future MRT lines which may cause housing prices to fall.

#### Splitting MRT/LRT Datasets

::: panel-tabset
## Filter

MRT

```{r}
geo_rail_mrt_above <- geo_railway_line %>% filter(GRND_LEVEL == "ABOVEGROUND") %>% filter(RAIL_TYPE == "MRT")
```

LRT

```{r}
geo_rail_lrt_above <- geo_railway_line %>% filter(GRND_LEVEL == "ABOVEGROUND") %>% filter(RAIL_TYPE == "LRT")
```

## Check

MRT

```{r}
glimpse(geo_rail_mrt_above)
```

LRT

```{r}
glimpse(geo_rail_lrt_above)
```
:::

#### Verifying MRT/LRT Aboveground Railway Line

```{r}
tmap_mode("plot") +
  tm_shape(mpsz) +
  tm_polygons("REGION_N", alpha = 0.05, border.alpha = 0.05) +
  tm_shape(geo_rail_mrt_above) +
  tm_lines("RAIL_TYPE", palette = "red") +
  tm_shape(geo_rail_lrt_above) +
  tm_lines("RAIL_TYPE", palette = "blue") +
  tm_layout(legend.position = c("right", "bottom"), 
            title= 'MRT/LRT Track Line in Singapore', 
            title.position = c('right', 'top'))
```

### Transform Parks Dataset

Let us view our parks dataset

```{r}
glimpse(geo_parks)
```

```{r}
tmap_mode("plot") +
  tm_shape(mpsz) +
  tm_polygons("REGION_N", alpha = 0.05, border.alpha = 0.05) +
  tm_shape(geo_parks) +
  tm_fill("darkgreen") +
  tm_layout(legend.position = c("right", "bottom"), 
            title= 'Parks in Singapore', 
            title.position = c('right', 'top'))
```

Firstly, as we recognise that parks comes in different shapes and sizes. Parks like Punggol Waterway Park are long by nature and spans the entire width of Punggol. Hence, using a Spatial Points by obtaining its centroid is not the most accurate as the entire length is a park. Hence, we opt to use the park polygon instead.

Our data is in the `MULTIPOLYGON` format. As we want to calculate the proximity from homes to the edges of parks, we need to convert it to `LINESTRING`. The code block uses *st_cast()* to help us cast the format from `MULTIPOLYGON` to `LINESTRING`

```{r}
geo_parks <- geo_parks %>% st_cast("MULTILINESTRING") %>% st_cast("LINESTRING")
```

Now, let us check and plot the map of the parks data.

```{r}
glimpse(geo_parks)
```

```{r}
tmap_mode("plot") +
  tm_shape(mpsz) +
  tm_polygons("REGION_N", alpha = 0.05, border.alpha = 0.05) +
  tm_shape(geo_parks) +
  tm_lines("darkgreen") +
  tm_layout(legend.position = c("right", "bottom"), 
            title= 'Parks in Singapore', 
            title.position = c('right', 'top'))
```

Great! We have successfully converted the data to `LINESTRING`!

### Prepare Good Primary Schools Dataset

[schlah.com](https://schlah.com/primary-schools) provides a good breakdown of factors that contributes to a school's ranking, based on the following extracted from their website:

-   Gifted Education Programme (GEP): 20%

-   Popularity in Primary 1 (P1) Registration: 20%

-   Special Assistance Plan (SAP): 15%

-   Singapore Youth Festival Arts Presentation: 15%

-   Singapore National School Games: 15%

-   Singapore Uniformed Groups Unit Recognition: 15%

In our analysis, we want to see if good schools can contribute to increased housing prices in Singapore. For our analysis, we will take that the top 10% (16) of primary schools in Singapore are 'good schools'

The code chunk below will extract the top 16 good primary schools for our analysis.

```{r}
TOP_10PCT_SCHS = c("NANYANG PRIMARY SCHOOL",
                  "TAO NAN SCHOOL",
                  "CATHOLIC HIGH SCHOOL",
                  "NAN HUA PRIMARY SCHOOL",
                  "ST. HILDA'S PRIMARY SCHOOL",
                  "HENRY PARK PRIMARY SCHOOL",
                  "ANGLO-CHINESE SCHOOL (PRIMARY)",
                  "RAFFLES GIRLS' PRIMARY SCHOOL",
                  "PEI HWA PRESBYTERIAN PRIMARY SCHOOL",
                  "CHIJ ST. NICHOLAS GIRLS' SCHOOL",
                  "ROSYTH SCHOOL",
                  "KONG HWA SCHOOL",
                  "POI CHING SCHOOL",
                  "HOLY INNOCENTS' PRIMARY SCHOOL",
                  "AI TONG SCHOOL",
                  "RED SWASTIKA SCHOOL")

geo_top_schools = geo_schools %>% filter(SCHOOLNAME %in% TOP_10PCT_SCHS)
glimpse(geo_top_schools)

```

There we have it, we have successfully extracted the top 10% of primary schools in Singapore (16 schools).

### Prepare CBD Outline

From [Wikipedia](https://en.wikipedia.org/wiki/Downtown_Core), we know that Singapore's CBD is also called `DOWNTOWN CORE`. To be accurate in our analysis, we will calculate the proximity to CBD based on the following rules:

-   if outside CBD boundary, we will calcualte the distance to the `LINESTRING`.

-   if within CBD, distance will be 0

The codeblock below achieves a few things:

1.  Filter to get the subzones of `DOWNTOWN CORE` planning area
2.  Combine the polygons to obtain the outline of `DOWNTOWN CORE` (CBD)
3.  Convert the geometry from `POLYGON` to `LINESTRING` format

```{r}
cbd_sf <- mpsz %>% filter(mpsz$PLN_AREA_N == "DOWNTOWN CORE")
cbd_geom <- st_union(cbd_sf)
cbd_geom <- st_cast(cbd_geom, "LINESTRING")
```

# Data Wrangling: Aspatial Data

We have three datasets that are Aspatial Data which only contains addresses of the locations. However, we cannot perform analysis without the coordinates of the datasets without its coordinates, hence, we need to geocode the data to retrieve its coordinates using onemap.

These are the datasets that require further processing:

-   CHAS Clinics

-   HDB HIP MUP

-   HDB Resale Flat Pricing

-   Shopping Malls

## Importing Aspatial Data

In the various tabs below, we will import each individual dataset from its respective folders, with a brief explanation of the use cases of each dataset.

::: panel-tabset
## CHAS Clinics

```{r}
CHAS_raw = read_xlsx("Take-Home_Ex03/aspatial/CHAS.xlsx") 
glimpse(CHAS_raw)
```

## HDB HIP MUP

```{r}
hdb_hip_mup_raw = read_xlsx("Take-Home_Ex03/aspatial/HDB_HIP-MUP-20230312.xlsx")
glimpse(hdb_hip_mup_raw)
```

## HDB Resale Flat Pricing

```{r}
hdb_resale_raw = read_csv("Take-Home_Ex03/aspatial/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv")
glimpse(hdb_resale_raw)
```

## Shopping Malls

```{r}
shopping_malls_raw = read_xlsx("Take-Home_Ex03/aspatial/malls-20230320.xlsx")
```
:::

## Filtering HDB Resale Flat Data

We will now filter the HDB Resale to focus on the target months, Jan 2020 to Feb 2023, and 5 Room HDBs to construct the predictive model. We will use:

-   *filter()* to filter out the desired room type and months

-   *unique()* to check if the desired room type and months has been filtered correctly

-   *glimpse()* to check the data structure of the filtered dataset

::: panel-tabset
## Filter Code

```{r}
hdb_resale <- filter(hdb_resale_raw, flat_type == "5 ROOM") %>%
              filter(month >= "2020-01" & month <= "2023-02")
```

## Glimpse Variables

```{r}
glimpse(hdb_resale)
```

## Unique Month and Flat_Type

```{r}
unique(hdb_resale$month)
```

```{r}
unique(hdb_resale$flat_type)
```
:::

From the code and results in the respective tabs (Glimpse Variables and Unique Month and Flat Type), we can see that:

-   There are **21,500 transactions** between Jan 2020 to Feb 2023.

-   The `month` and `flat_type` has been extracted correctly.

## Transforming Aspatial Data - Create New Columns with Values

Next, we transform the Aspatial Datasets into more meaningful values:

1.  CHAS Clinics - There is nothing to transform, since as noted earlier, there is already a `postal` column provided
2.  HDB HIP MUP - We need to obtain the address for geocoding (obtaining the SVY21 `X` and `Y` coordinates) by combining the `BLK` and `STREET` fields
3.  HDB Resale Flat Pricing - We need to obtain the address for geocoding (obtaining the SVY21 `X` and `Y` coordinates) by combining the `block` and `street_name` fields, and also convert the remaining lease from the form of `YY years MM months` to a more machine-readable format (ie. `MM` months)
4.  Shopping Malls - Nothing to transform, we can use the `Mall_Name` as the search term to obtain the geocode (SVY `X` and `Y` coordinates)

The code chunks will assist with the transformation using *mutate()*Â further explained below:

::: panel-tabset
## HDB HIP MUP

We *mutate()* the `hdb_hip_mup_raw` dataset by pasting the `BLK` and `STREET` columns together into the `address` column to a new sf dataframe called `hdb_hip_mup_trans`

```{r}
hdb_hip_mup_trans <- hdb_hip_mup_raw %>%
  mutate(hdb_hip_mup_raw, address = paste(BLK, STREET))
```

## HDB Resale Flat Pricing

We *mutate()* the `hdb_resale` dataset by pasting the `block` and `street_name` columns together into the `address` column to a new variable called `hdb_hip_mup_trans`. We also used *mutate()* to modify the existing `remaining_lease` data to the form of `MM`.

The first section of the code `as.integer(str_sub(remaining_lease, 0, 2)) * 12` extracts the year numbers as `YY` and converts it into string and then multiplying it by 12 to convert it to number of months.

The next part of the code checks if there is any numerical `MM` (month) present, if there is no month present, the value will be `NA` and 0 will be assigned in place of `NA`. Else, if present, we take the `MM`.

The integer month is summed with the year in months to form this column `remaining_lease_mths` in the new sf dataframe `hdb_resale_trans`

```{r}
hdb_resale_trans <- hdb_resale %>%
  mutate(hdb_resale, address = paste(block, street_name)) %>%
  mutate(hdb_resale, remaining_lease_mths = (as.integer(str_sub(remaining_lease, 0, 2)) * 12 + ifelse(is.na(as.integer(str_sub(remaining_lease, 9, 11))), 0,  as.integer(str_sub(remaining_lease, 9, 11)))))
```

Next, let us left join the HDB HIP MUP data into HDB Resale Transactions so that we know which HDB units have already completed their upgrading.

```{r}
hdb_resale_trans <- left_join(hdb_resale_trans, hdb_hip_mup_trans)
glimpse(hdb_resale_trans)
```

Then we'll select only the unnecessary columns:

```{r}
hdb_resale_trans <- hdb_resale_trans %>% select(c(1:13, 16))
```
:::

## Retrieving SVY21 Coordinate of Addresses

This section will focus on retrieving relavant data such as coordinates of the address which we could use in further spatial analysis to obtain proximity to locational factors later.

We are interested in obtaining the `SVY21` `X` and `Y` coordinates as they are in the Projected Coordinate System, which allows us to perform measure directly without any additional transformations.

### Create a List Storing Unique Addresses/Postal Codes

Since some addresses/postal codes are duplicated, we store and check unique addresses to reduce the amount of `GET` requests sent to the OneMap API:

1.  Faster
2.  OneMap API has a rate limit of 250 API calls a minute
3.  It makes it easier for us to locate errors and correct it

Here, we will obtain a list of unique addresses/postal codes for each data set.

::: panel-tabset
## CHAS Clinics

```{r}
addr_lst.chas <- sort(unique(CHAS_raw$Postal))
glimpse(addr_lst.chas)
```

## HDB Resale Flat Pricing

```{r}
addr_lst.resale <- sort(unique(hdb_resale_trans$address))
glimpse(addr_lst.resale)
```

## Shopping Malls

```{r}
addr_lst.malls <- sort(unique(shopping_malls_raw$Mall_Name))
glimpse(addr_lst.malls)
```
:::

### Create Function to Retrieve Coordinates from OneMap.sg API

The following function uses OneMap.sg Search API to obtain coordinates (SVY21 X, Y) using part of an address or postal code.

This is how the function `get_coordinates()` below will work:

1.  `new_coords` datafame is created to store all the new coordinate data and its original address that is input to the GET request API
2.  for each `addr` in `addr_lst` where `addr_lst` is the list passed into the function, we will query each record and append accordingly:
    1.  If there is 1 or more records, we append the top record's SVY21 X, Y coordinates and `addr` to a temporary dataframe called `new_row`,

    2.  Else, `NA` for it's `X` and `Y` columns and the `addr` is stored in `new_row`.
3.  The GET Request has various parameters:
    1.  `searchVal` - the value to pass to OneMap Search to obtain the Geocode (in this case we are interested in SVY21 X, Y coordinates)

    2.  `returnGeom` - return details about geometry (ie. SVY21 X, Y or Lat Lon), `Y` in this case as we want SVY21 X, Y coordinates

    3.  `getAddrDetails` - get more details about the address, `N` in this case as we don't require further information.
4.  fromJSON() helps us convert the JSON format to a list format for manipulation
    1.  the function rawToChar() was used as the received type for `reply$content`Â is RAW, which requires conversion before we can read the values
5.  Lastly, we will combine the `new_row` data into the main `new_coords` dataframe using *rbind()* as they are both dataframes.

<div>

```{r}
get_coordinates <- function(addr_lst){
  
  # Create a data frame to store all retrieved coordinates
  new_coords <- data.frame()
    
  for (addr in addr_lst){
    #print(i)

    reply <- GET('https://developers.onemap.sg/commonapi/search?',
           query = list(searchVal = addr,
                        returnGeom = 'Y',
                        getAddrDetails = 'N'))
    
    output <- fromJSON(rawToChar(reply$content))
    found <- output$found
    res <- output$results
    
    # Create a new data frame for each address
    new_row <- data.frame()
    
    # If single result, append 
    if (found >= 1){
      res_1 <- head(res, n = 1)
      x <- res_1$X
      y <- res_1$Y
      new_row <- data.frame(address = addr, x = x, y = y)
    }

    else {
      new_row <- data.frame(address = addr, x = NA, y = NA)
    }
    
    # Add the row
    new_coords <- rbind(new_coords, new_row)
  }
  return(new_coords)
}
```

</div>

### Call get_coordinates() Function to Obtain Coordinates

We use *get_coordinates()* function created earlier to obtain the coordinates of the address. *glimpse()* allows us to view and check if the data has been properly created.

RDS Scripts contains scripts to import/export the coordinates R objects to RDS file format (R Data Serialisation) prevent having to call the API each time on every render.

::: panel-tabset
## get_coordinates() Function

CHAS Clinics

```{r eval=FALSE}
coords_chas <- get_coordinates(addr_lst.chas)
```

HDB Resale Flat Pricing

```{r eval=FALSE}
coords_resale <- get_coordinates(addr_lst.resale)
```

Shopping Malls

```{r eval=FALSE}
coords_malls <- get_coordinates(addr_lst.malls)
```

## RDS Scripts

Writing RDS

```{r eval=FALSE}
write_rds(coords_chas, "Take-Home_Ex03/rds/coords_chas.rds")
write_rds(coords_resale, "Take-Home_Ex03/rds/coords_resale.rds")
write_rds(coords_malls, "Take-Home_Ex03/rds/coords_malls.rds")
```

Reading RDS

```{r}
coords_chas <- read_rds("Take-Home_Ex03/rds/coords_chas.rds")
coords_resale <- read_rds("Take-Home_Ex03/rds/coords_resale.rds")
coords_malls <- read_rds("Take-Home_Ex03/rds/coords_malls.rds")
```

## Glimpse Records

CHAS Clinics

```{r}
glimpse(coords_chas)
```

HDB Resale Flat Pricing

```{r}
glimpse(coords_resale)
```

Shopping Malls

```{r}
glimpse(coords_malls)
```
:::

## Data Verification for Coordinate Data

With the retrieved data, we need to inspect and verify the data received and correct any errors made along the way. We will do all the steps in parallel for each dataset, outlined in step format below:

1.  Merge coordinate data and original dataframe
    -   We do this as the CHAS Clinics coordinates are derived from Postal Code and it might be hard to figure out which place are we looking at by looking at just the postal code
2.  Check for `NA` X/Y values and manually amend if required
3.  Convert DataFrame into a sf Object
4.  Plot a tmap and check if points are plotted in the correct regions

At any step if there are issues, we will detail steps to fix or recover from it.

### CHAS Clinics

1.  Merge Coordinate Data and Original Dataframe

    ```{r}
    temp_chas <- left_join(CHAS_raw, coords_chas, by=c("Postal" = "address"))
    ```

2.  Check for `NA` X/Y values and manually amend if required

    ```{r}
    filter(temp_chas, is.na(x) == TRUE)
    ```

    Here, using *filter()* and *is.na()*, we find out which records do not have a valid location assigned to it. Now, let us manually check through the records and fix the issue.

    +-------------------------------------------------------------------------+------------------------------------------------------------------------------+
    | CHAS Clinic Address                                                     | Issue                                                                        |
    +=========================================================================+==============================================================================+
    | 189, Selegie Road, Selegie Centre, #01- 05, Singapore 188332            | No longer exists based on Onemap and Google Map, we will remove it           |
    +-------------------------------------------------------------------------+------------------------------------------------------------------------------+
    | 140, Corporation Drive, #01- 03                                         | Postal Code number **610140** according to OneMap, we will amend accordingly |
    +-------------------------------------------------------------------------+------------------------------------------------------------------------------+
    | 6, Gemmill Lane                                                         | Postal Code number **069249** according to OneMap, we will amend accordingly |
    +-------------------------------------------------------------------------+------------------------------------------------------------------------------+
    | 102, Yishun Avenue 5, #01- 133, Singapore\\r\\n760102                   | No longer exists based on Onemap and Google Map, we will remove it           |
    +-------------------------------------------------------------------------+------------------------------------------------------------------------------+
    | 34, Craig Road, Chinatown Plaza, #01- 04,\\r\\nSingapore 089673         | No longer exists based on Onemap and Google Map, we will remove it           |
    +-------------------------------------------------------------------------+------------------------------------------------------------------------------+
    | 1, Rochor Road, Rochor Centre, #03- 516,\\r\\nSingapore 180001          | No longer exists based on Onemap and Google Map, we will remove it           |
    +-------------------------------------------------------------------------+------------------------------------------------------------------------------+
    | 51, TAMPINES AVENUE 4, OUR TAMPINES\\r\\nHUB, #B1- 04/05                | Records are appended as **528523** on OneMap, we will amend accordingly      |
    +-------------------------------------------------------------------------+------------------------------------------------------------------------------+
    | 50, Market Street, Golden Shoe Car Park,\\r\\n#01- 30, Singapore 048940 | No longer exists based on OneMap and Google Map, we will remove it           |
    +-------------------------------------------------------------------------+------------------------------------------------------------------------------+

    Now, let us update:

    #### Fixing Data

    ::: panel-tabset
    ## 1. Update Records

    We remove the clinics that are non-existent using *filter()*

    ```{r}
    chas_updated <- filter(CHAS_raw, !Address %in%
      c("189, Selegie Road, Selegie Centre, #01- 05,\r\nSingapore 188332",
        "102, Yishun Avenue 5, #01- 133, Singapore\r\n760102",
        "34, Craig Road, Chinatown Plaza, #01- 04,\r\nSingapore 089673",
        "1, Rochor Road, Rochor Centre, #03- 516,\r\nSingapore 180001",
        "50, Market Street, Golden Shoe Car Park,\r\n#01- 30, Singapore 048940"))
    ```

    Next, we use *mutate()*Â and *ifelse()* condition to update the Postal Codes of the clinics at the relavant addresses.

    ```{r}
    chas_updated <- chas_updated %>% 
      mutate(Postal = ifelse(Address == "140, Corporation Drive, #01- 03", "610140", Postal)) %>%
      mutate(Postal = ifelse(Address == "6, Gemmill Lane", "069249", Postal)) %>%
      mutate(Postal = ifelse(Address == "51, TAMPINES AVENUE 4, OUR TAMPINES\r\nHUB, #B1- 04/05", "528523", Postal))
    ```

    Lastly, we regenerate the list of unique Postal Codes to be geocoded.

    ```{r}
    addr_lst.chas_upd <- sort(unique(chas_updated$Postal))
    glimpse(addr_lst.chas_upd)
    ```

    ## 2. Rerun get_coordinates()

    We get the SVY21 X,Y coordinates using our *get_coordinates()* function

    ```{r eval=FALSE}
    coords_chas_upd <- get_coordinates(addr_lst.chas_upd)
    ```

    Saving the DataFrame as .rds for future use to prevent rerunning *get_coordinates()* GET API everytime a render is run

    ```{r eval=FALSE}
    write_rds(coords_chas_upd, "Take-Home_Ex03/rds/coords_chas_upd.rds")

    ```

    Load the DataFrame from .rds

    ```{r}
    coords_chas_upd <- read_rds("Take-Home_Ex03/rds/coords_chas_upd.rds")

    ```

    ## 3. Combine DataFrame and verify

    We left join the `chas_updated` main table and coordinates and filter the `x` column for any null values

    ```{r}
    temp_chas <- left_join(chas_updated, coords_chas_upd, by=c("Postal" = "address"))
    filter(temp_chas, is.na(x) == TRUE)
    ```

    No null values found, we have completed this step!
    :::

3.  Convert a DataFrame into a sf Object

    We specify the SVY21 X and Y coordinates to be used as the coordinate geometry. The `crs` specified is `3414` which refers to `SVY21`.

    ```{r}
    chas_sf <- st_as_sf(temp_chas,
                            coords = c("x", "y"),
                            crs = 3414)
    ```

4.  Plot a tmap and check if points are plotted in the correct regions

    Now, we will plot an interactive tmap to check if our points are correct.

    ```{r}
    tmap_mode("view")
    tm_shape(chas_sf) +
      tm_dots("Type",
              popup.vars=c("Name"="Name", "Address"="Address", "Type" = "Type", "Telephone" = "Telephone"))
    ```

    From our analysis, the points looks to be correctly located.

### HDB Resale Flat Pricing

1.  Merge Coordinate Data and Original Dataframe

    ```{r}
    temp_hdb_resale_trans <- left_join(hdb_resale_trans, coords_resale, by=c("address" = "address"))
    ```

2.  Check for `NA` X/Y values and manually amend if required

    ```{r}
    filter(temp_hdb_resale_trans, is.na(x) == TRUE)
    ```

    No `NA` values, great!

3.  Convert a DataFrame into a sf Object

    We specify the SVY21 X and Y coordinates to be used as the coordinate geometry. The `crs` specified is `3414` which refers to `SVY21`.

    ```{r}
    hdb_resale_sf <- st_as_sf(temp_hdb_resale_trans,
                            coords = c("x", "y"),
                            crs = 3414)
    ```

4.  Plot a tmap and check if points are plotted in the correct regions

    Now, we will plot an interactive tmap to check if our points are correct. We overlay the URA Master Plan Regions for a quick overlay to roughly check if the HDBs are located in the correct areas. Do note that HDB Towns differ from URA Planning Areas.

    Generate external interactive plot

    ```{r eval=FALSE}
    tmap_mode("plot")
    hdb_plot1 <- tm_shape(mpsz) +
      tm_polygons("REGION_N",
                  alpha = 0.5) +
    tm_shape(hdb_resale_sf) +
      tm_dots("town",
              popup.vars=c("block"="block", "street_name"="street_name", "flat_model" = "flat_model", "town" = "town", "resale_price" = "resale_price", "remaining_lease_mths", "remaining_lease_mths"))
    tmap_save(hdb_plot1, "thex03_hdbplot1.html")
    ```

    Static Plot

    ```{r}
    tmap_mode("plot")
    tm_shape(mpsz) +
      tm_polygons("REGION_N",
                  alpha = 0.5) +
    tm_shape(hdb_resale_sf) +
      tm_dots("town", size = 0.02)
    ```

    [View Interactive Version of Map here](thex03_hdbplot1.html){target="_blank"}! *\[20+mb\]*

    Oddly, 27 Marine Cres appeared as a point on Sembcorp Marine Tuas Crescent and 54 Kent Rd somehow appeared as a point on 54J SOUTH BUONA VISTA ROAD KENT RIDGE HILL RESIDENCES. There are also some other differences, so let us now recode some of the addresses to get them to the right locations:

    #### Fixing Data

    ::: {.panel-tabset .column-screen-inset-left}
    ## 1. Update Records

    We use *mutate()* to replace the existing addresses with more specific ones that we found on OneMap.

    ```{r}
    mod_hdb_resale_trans <- hdb_resale_trans %>% 
      mutate(address = ifelse(address == "10 JLN BATU", "10 JALAN BATU DI TANJONG RHU", address)) %>%
      mutate(address = ifelse(address == "11 JLN BATU", "11 JALAN BATU DI TANJONG RHU", address)) %>%     
      mutate(address = ifelse(address == "54 KENT RD", "54 KENT ROAD KENT VILLE", address)) %>%    
      mutate(address = ifelse(address == "27 MARINE CRES", "27 MARINE CRESCENT MARINE CRESCENT VILLE", address))
    ```

    ```{r}
    temp_hdb_resale_trans <- left_join(mod_hdb_resale_trans, coords_resale, by=c("address" = "address"))
    ```

    Lastly, we regenerate the list of unique Postal Codes to be geocoded.

    ```{r}
    addr_lst.resale_upd <- sort(unique(mod_hdb_resale_trans$address))
    glimpse(addr_lst.resale_upd)
    ```

    ## 2. Rerun get_coordinates()

    We get the SVY21 X,Y coordinates using our *get_coordinates()* function

    ```{r eval=FALSE}
    coords_resale_upd <- get_coordinates(addr_lst.resale_upd)
    ```

    Saving the DataFrame as .rds for future use to prevent rerunning *get_coordinates()* GET API everytime a render is run

    ```{r eval=FALSE}
    write_rds(coords_resale_upd, "Take-Home_Ex03/rds/coords_resale_upd.rds")

    ```

    Load the DataFrame from .rds

    ```{r}
    coords_resale_upd <- read_rds("Take-Home_Ex03/rds/coords_resale_upd.rds")

    ```

    ## 3. Combine DataFrame and verify

    We left join the `hdb_hip_mup_trans_upd` main table and coordinates and filter the `x` column for any null values

    ```{r}
    temp_hdb_resale_trans <- left_join(mod_hdb_resale_trans, coords_resale_upd, by=c("address" = "address"))
    filter(temp_hdb_resale_trans, is.na(x) == TRUE)
    ```

    No null values found, we have completed this step!

    ## 4. Convert a DataFrame into a sf Object

    ```{r}
    hdb_resale_sf <- st_as_sf(temp_hdb_resale_trans,
                            coords = c("x", "y"),
                            crs = 3414)
    ```

    ## 5. Plot a tmap and check if points are plotted in the correct regions

    Generate external interactive map

    ```{r eval=FALSE}
    tmap_mode("plot")
    hdb_plot2 <- tm_shape(mpsz) +
      tm_polygons("REGION_N",
                  alpha = 0.5) +
    tm_shape(hdb_resale_sf) +
      tm_dots("town",
              popup.vars=c("block"="block", "street_name"="street_name", "flat_model" = "flat_model", "town" = "town", "resale_price" = "resale_price", "remaining_lease_mths", "remaining_lease_mths"))
    tmap_save(hdb_plot2, "thex03_hdbplot2.html")
    ```

    Static Plot

    ```{r}
    tmap_mode("plot")
    tm_shape(mpsz) +
      tm_polygons("REGION_N",
                  alpha = 0.5) +
    tm_shape(hdb_resale_sf) +
      tm_dots("town")
    ```

    [View Interactive Version of Map here](thex03_hdbplot2.html){target="_blank"}! *\[20+mb\]*

    Great! All the blocks looks to be plotted in the correct locations!
    :::

### Shopping Malls

1.  Merge Coordinate Data and Original Dataframe

    ```{r}
    temp_malls <- left_join(shopping_malls_raw, coords_malls, by=c("Mall_Name" = "address"))
    ```

2.  Check for `NA` X/Y values and manually amend if required

    ```{r}
    filter(temp_malls, is.na(x) == TRUE)
    ```

    No `NA` values, great!

3.  Convert a DataFrame into a sf Object

    We specify the SVY21 X and Y coordinates to be used as the coordinate geometry. The `crs` specified is `3414` which refers to `SVY21`.

    ```{r}
    geo_malls <- st_as_sf(temp_malls,
                            coords = c("x", "y"),
                            crs = 3414)
    ```

4.  Plot a tmap and check if points are plotted in the correct regions

    Now, we will plot an interactive tmap to check if our points are correct. We overlay the URA Master Plan Regions for a quick overlay to roughly check if the malls are located in the correct areas. Do note that mall region may differ from URA Planning Areas.

    ```{r}
    tmap_mode("view")
    tm_shape(mpsz) +
      tm_polygons("REGION_N",
                  alpha = 0.5) +
    tm_shape(geo_malls) +
      tm_dots("Region",
              popup.vars=c("Mall_Name"="Mall_Name", "Region"="Region"),
              size = 0.05,
              palette = "Set2")
    ```

    Nice! All the malls seems to be in their right locations.

# Preparing Locational Factors

From our list of locational factors, we can see that in general, we have two types of locational factors:

1.  Count of a factor within a certain radius
2.  Proximity of housing to a factor

We have created functions below that will prepare out data we require for our analysis.

## Functions

### Get Proximity Locational Factors

The `get_prox()` function below takes in an origin and destination dataframe and creates a distance matrix of origin and destination pairs based on *st_distance().* Next, we use mutate and apply to locate the destination that is located the minimum distance away from the origin and save it to the corresponding row in `origin_df` under the `PROX` column. The `1`Â in apply is to apply the function row by row, which corresponds directly to the `origin_df` rows.

Next, we rename the columns based on whats specified by the input parameter and return the dataframe.

For the code below, we will use it to detect for 2 types of spatial types for destinations, Points and Linestring.

For our two linestring datasets, we know that there are:

1.  No HDBs within park boundaries
2.  No HDBs within Downtown Core

Hence, the following datasets will only need to factor distance to the boundary and there are no concerns that there are HDBs within those regions.

```{r}
get_prox <- function(origin_df, dest_df, col_name){
  
  # creates a matrix of distances
  dist_matrix <- st_distance(origin_df, dest_df)           
  
  # find the nearest location_factor and create new data frame
  near <- origin_df %>% 
    mutate(PROX = apply(dist_matrix, 1, function(x) min(x)) / 1000) 
  
  # rename column name according to input parameter
  names(near)[names(near) == 'PROX'] <- col_name

  # Return df
  return(near)
}
```

### Get Num Within Locational Factors

The `get_within()` function below takes in an origin and destination dataframe and creates a distance matrix of origin and destination pairs based on *st_distance().*

Next, we use mutate and apply to obtain the sum of destinations that fits less than or equal to the `threshold_dist` specified and save the sum value to the corresponding row in `origin_df` under the `PROX` column. The `1`Â in apply is to apply the function row by row, which corresponds directly to the `origin_df` rows.

Next, we rename the columns based on whats specified by the input parameter and return the dataframe.

```{r}
get_within <- function(origin_df, dest_df, threshold_dist, col_name){
  
  # creates a matrix of distances
  dist_matrix <- st_distance(origin_df, dest_df)   
  
  # count the number of location_factors within threshold_dist and create new data frame
  wdist <- origin_df %>% 
    mutate(WITHIN_DT = apply(dist_matrix, 1, function(x) sum(x <= threshold_dist)))
  
  # rename column name according to input parameter
  names(wdist)[names(wdist) == 'WITHIN_DT'] <- col_name

  # Return df
  return(wdist)
}
```

## Generating Locational Factors and Saving Results

::: panel-tabset
## Generating Proximity Locational Factors

Using the code chunk below, we will generate the proximity to locational factors specified below:

```{r eval=FALSE}
geo_hdb_resale <- hdb_resale_sf
geo_hdb_resale <- get_prox(geo_hdb_resale, cbd_geom, "PROX_CBD")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_eldercare, "PROX_ELDER")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_hawker, "PROX_HAWKER")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_mrt_existing, "PROX_MRT_E")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_parks, "PROX_PARK")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_top_schools, "PROX_TOP_SCH")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_malls, "PROX_MALL")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_supermarkets, "PROX_SUPMKT")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_rail_mrt_above, "PROX_TRK_MRT")
geo_hdb_resale <- get_prox(geo_hdb_resale, geo_rail_lrt_above, "PROX_TRK_LRT")
```

## Generating Num Within Locational Factors

Now, using the code chunk below, we will obtain the count of location factors specified:

```{r eval=FALSE}
geo_hdb_resale <- get_within(geo_hdb_resale, geo_childcare, 350, "NUM_350_CHILD")
geo_hdb_resale <- get_within(geo_hdb_resale, geo_kindergartens, 350, "NUM_350_KINDER")
geo_hdb_resale <- get_within(geo_hdb_resale, geo_bus_stop, 350, "NUM_350_BUS")
geo_hdb_resale <- get_within(geo_hdb_resale, geo_schools, 1000, "NUM_1000_SCH")
geo_hdb_resale <- get_within(geo_hdb_resale, geo_mrt_future, 800, "NUM_800_MRT_F")
geo_hdb_resale <- get_within(geo_hdb_resale, geo_lrt, 350, "NUM_350_LRT")
```

## RDS Scripts

To prevent running the code above again on every render, we will save the results to a RDS file. We will use the read script to read the file without having to rerun the processing on every render.

```{r eval=FALSE}
write_rds(geo_hdb_resale, "Take-Home_Ex03/rds/geo_hdb_resale.rds")
```

```{r}
geo_hdb_resale <- read_rds("Take-Home_Ex03/rds/geo_hdb_resale.rds")
```
:::

```{r}
#tmap_mode("view")
#tm_shape(hdb_resale) +
#  tm_dots("resale_price",
#      
#popup.vars=c("month"="month", "town"="town", "block" = "block", "street_name" = "street_name"))
```

# Preparing Structural Factors

Looking at the list of structural factors, there are some factors that requires further processing. These strucutral factors are listed in the table below.

+----------------------------------------+-------------+---------------------------------------------------------------------------------------------------------------------------------+
| dName of Structural Factor             | Data Type   | Remarks                                                                                                                         |
+========================================+=============+=================================================================================================================================+
| Area of Unit                           | Numerical   |                                                                                                                                 |
+----------------------------------------+-------------+---------------------------------------------------------------------------------------------------------------------------------+
| Floor Level                            | Categorical | Requires recoding of values                                                                                                     |
+----------------------------------------+-------------+---------------------------------------------------------------------------------------------------------------------------------+
| Remaining Lease                        | Numerical   | Data has been processed to numerical readable values in months in [Transforming Aspatial Data - Create New Columns with Values] |
+----------------------------------------+-------------+---------------------------------------------------------------------------------------------------------------------------------+
| Age of the unit                        | Numerical   |                                                                                                                                 |
+----------------------------------------+-------------+---------------------------------------------------------------------------------------------------------------------------------+
| Main Upgrading Program (MUP) Completed | Categorical | Requires one-hot encoding                                                                                                       |
+----------------------------------------+-------------+---------------------------------------------------------------------------------------------------------------------------------+
| Apartment Model                        | Categorical | This data has to be derived and standardised from `flat_model`                                                                  |
+----------------------------------------+-------------+---------------------------------------------------------------------------------------------------------------------------------+
| Apartment Multi-story                  | Categorical | This data has to be derived and standardised from `flat_model`                                                                  |
+----------------------------------------+-------------+---------------------------------------------------------------------------------------------------------------------------------+

## Floor Level

Now, let us first look at the floor levels.

```{r}
storeys <- sort(unique(geo_hdb_resale$storey_range))
storeys
```

From the unique values obtained above, we can see that story range is provided as a categorical range of every three floors. In the data above, we can see that there are 17 storey range categories.

Let us recode the categorical naming to numerical values by assigning 1 to the first range `01 TO 03` and 17 to the last range `49 TO 51`.

```{r}
storey_order <- 1:length(storeys)
storey_range_order <- data.frame(storeys, storey_order)
storey_range_order
```

From our data frame above, we have obtained the storey ranges and `storey_order`. Using the code below, we will use left_join to join `storey_order` to the main `geo_hdb_resale` dataframe.

```{r}
geo_hdb_resale <- left_join(geo_hdb_resale,  storey_range_order, by = c("storey_range" = "storeys"))
```

There we go, we have combined the recorded storey range values as `storey_order`.

## HDB Apartment Model and Multi-storey

Not all HDB Apartments are built the same, there are different HDB Models and some HDB units are multi-storey. Let us explore what kinds of models do we have in our dataset:

```{r}
unique(geo_hdb_resale$flat_model)
```

From our data above, we can see that we have 11 distinct categories of HDB Apartment Types. Some of these terminologies changed over time and may refer to the configuration or whether the apartments came with furnishings.

Let use understand what some of the terms [mean](https://www.teoalida.com/singapore/hdbflattypes/):

-   Design Build Sell Scheme (DBSS) flats may call for a higher value than regular HDB flats as they are designed, build and sold by 3rd party developers although they are still HDB Flats. They are supposed to be better than premium flats

-   Premium flats which come with pre-installed fittings and furnishings over standard apartments which comes with none

-   Standard flats are opposite of premium, they don't come with furnishings or fittings

-   Maisonette / Loft comes with a second floor of apartment space

-   3Gen is a new type of single-key unit with additional bedroom and bathroom for grandparents to live in

-   Adjoined flats are units where two HDB units are combined (may or may not have 2 front doors)

-   Type S2 are types assigned to 5-room units in The Pinnacle at Duxton

Let us recode them so that the model can generalise better.

::: panel-tabset
## Recode Values

+------------------------+---------------------------------------+------------------------------+
| Original Values        | Recoded Values (model\_\<valuename\>) | Recoded Values (multistorey) |
+========================+=======================================+==============================+
| Improved               | Standard = 1                          | 0                            |
+------------------------+---------------------------------------+------------------------------+
| Standard               | Standard = 1                          | 0                            |
+------------------------+---------------------------------------+------------------------------+
| DBSS                   | DBSS = 1                              | 0                            |
+------------------------+---------------------------------------+------------------------------+
| Model A                | Standard = 1                          | 0                            |
+------------------------+---------------------------------------+------------------------------+
| Adjoined flat          | Adjoined = 1                          | 0                            |
+------------------------+---------------------------------------+------------------------------+
| Premium Apartment      | Premium = 1                           | 0                            |
+------------------------+---------------------------------------+------------------------------+
| Type S2                | S2 = 1                                | 0                            |
+------------------------+---------------------------------------+------------------------------+
| Model A-Maisonette     | Standard = 1                          | 1                            |
+------------------------+---------------------------------------+------------------------------+
| Premium Apartment Loft | Premium = 1                           | 1                            |
+------------------------+---------------------------------------+------------------------------+
| Improved - Maisonette  | Standard = 1                          | 1                            |
+------------------------+---------------------------------------+------------------------------+
| 3Gen                   | 3Gen = 1                              | 0                            |
+------------------------+---------------------------------------+------------------------------+

## Create Multistorey Columns

Using the code chunk below, we will check if the `flat_model` corresponds to the following types, if it is, we code the value in the `multistorey` as 1. Else, 0 is assigned.

```{r}
geo_hdb_resale <- geo_hdb_resale %>% mutate(multistorey = ifelse(flat_model %in% c("Improved-Maisonette", "Model A-Maisonette", "Premium Apartment Loft"), 1, 0 ))
```

## Recode

```{r}
geo_hdb_resale <- geo_hdb_resale %>% mutate(model_standard = ifelse(flat_model %in% c("Improved", "Standard", "Model A", "Model A-Maisonette", "Improved-Maisonette"), 1, 0))
geo_hdb_resale <- geo_hdb_resale %>% mutate(model_premium = ifelse(flat_model %in% c("Premium Apartment", "Premium Apartment Loft"), 1, 0))
geo_hdb_resale <- geo_hdb_resale %>% mutate(model_dbss = ifelse(flat_model %in% c("DBSS"), 1, 0))
geo_hdb_resale <- geo_hdb_resale %>% mutate(model_adjoined = ifelse(flat_model %in% c("Adjoined"), 1, 0))
geo_hdb_resale <- geo_hdb_resale %>% mutate(model_3gen = ifelse(flat_model %in% c("3Gen"), 1, 0))
geo_hdb_resale <- geo_hdb_resale %>% mutate(model_s2 = ifelse(flat_model %in% c("S2"), 1, 0))
```

Nice, we have completed our recoding, let us view a snippet of our data

## Glimpse

```{r}
glimpse(geo_hdb_resale)
```
:::

## HDB HIP MUP

Similarly for HIP MUP data, since there are all coded as `HIP` or `MUP` or NA cateogrical values, we need to convert them to numbers so that the model will be able to build.

In this case, we will create two new columns, `HIP` and `MUP` to track which kind of upgrading project has been done on the unit.

::: panel-tabset
## Recode

Using the code chunk, we will recode the respective values into the respective columns as 1 (true) or 0 (false).

```{r}
geo_hdb_resale <- geo_hdb_resale %>% mutate(hip = ifelse(is.na(TYPE), 0, ifelse(TYPE == "HIP", 1, 0)))
geo_hdb_resale <- geo_hdb_resale %>% mutate(hip = ifelse(is.na(TYPE), 0, ifelse(TYPE == "MUP", 1, 0)))
```

The manipulation is complete, let us glimpse the values.

## Glimpse

```{r}
glimpse(geo_hdb_resale)
```
:::

## RDS Scripts and Preparing for EDA

We will now save our prepared HDB Resale dataset and mpsz to a RDS file and clear all variables to free up the memory before reloading the dataset as `final_resale`.

The `rm(list=ls())` function will clear all variables

```{r eval=FALSE}
write_rds(geo_hdb_resale,"Take-Home_Ex03/rds/final_resale.rds")
write_rds(mpsz,"Take-Home_Ex03/rds/mpsz.rds")
```

```{r}
rm(list=ls())
```

```{r}
final_resale <- read_rds("Take-Home_Ex03/rds/final_resale.rds")
mpsz <- read_rds("Take-Home_Ex03/rds/mpsz.rds")
```

# Exploratory Data Analysis

Now, we can perform EDA on our prepared dataset `geo_hdb_resale` to better understand our dataset.

```{r}
glimpse(final_resale)
```

## Removing Columns Not Required For Analysis

Firstly, let us remove the columns that are not required for further analysis to save on memory space.

To do this, we use the *select()* function and indicate the columns we want to remove by having the prefix`-`

```{r}
final_resale <- select(final_resale, c(-flat_type,-storey_range,-flat_model, -lease_commence_date, -address, -TYPE, -remaining_lease))
```

Now, let us view the columns remaining.

```{r}
glimpse(final_resale)
```

## Understanding Resale Prices

Now, let us plot a histogram to understand the pricing of 5-Room resale flats between Jan 2020 to Feb 2023.

```{r}
ggplot(final_resale, aes(x=resale_price)) +
  geom_histogram(bins = 20, color = "black", fill = "lightblue")
```

From our graph above, we can see that:

-   Right-skewed distribution of resale_prices

-   Most resale HDBs are transacted near the \$500,000 range.

-   Outliers are seen where HDB prices are transacted near \$1 million or more

In this scenerio, while we can take log of `resale_price`, we will not perform the transformation to make it a normal distribution as:

1.  `resale_price` is the value to be predicted, we do not want to predict the log of `resale_price`
2.  Using the log of `resale_price` will cause it to have a high correlation with actual `resale_price`

## Understanding Structural Factors

## Understanding Locational Factors

## Statistical Point Map

Let

```{r eval=FALSE}
tmap_options("view")
tm_shape(mpsz) +
  tm_polygons("REGION_N",
              alpha = 0.5) +
tm_shape(final_resale) +
  tm_dots("resale_price",
          popup.vars= TRUE,
          size = 0.05,
          palette = "Set2")
```

# Credits

https://stackoverflow.com/questions/50775357/how-to-read-in-kml-file-properly-in-r-or-separate-out-lumped-variables-into-col

https://www.teoalida.com/singapore/hdbflattypes/
